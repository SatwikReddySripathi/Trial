this is the original


Title: “From Hallucinations to Correctness: Redefining QA Evaluation with LLM-as-a-Judge”

⸻

1. Setting the Stage — Why This Problem Exists

“In any real-world deployment of QA systems, especially in domains like enterprise search or customer support, correctness isn’t just a luxury—it’s a business necessity.”

We were tasked with evaluating an external QA tool against our internal QA system. The goal wasn’t just to compare performance superficially, but to understand how each system handled truth, nuance, and ambiguity — not just whether they answered, but how well they answered, and how truthfully.

The traditional approach? Rely on BLEU, ROUGE, or exact match metrics. But here’s where the real problem started.

⸻

2. The Problem — Standard Metrics Are Blind to Semantics and Hallucinations

“The real world doesn’t speak in exact n-grams.”

We noticed early on that:
	•	A semantically accurate answer could be penalized if it didn’t match the ground truth textually.
	•	Hallucinated answers, ones that sound plausible but fabricate information, could slip by undetected.
	•	Partially correct or over-informative answers (that go beyond the question but stay factually grounded) had no fair way of being recognized.

This makes traditional evaluation:
	•	Insensitive to meaning
	•	Unable to detect hallucinations
	•	Incapable of rewarding completeness or penalizing unnecessary additions

⸻

3. Challenges in Redefining Evaluation

We didn’t just need a new metric—we needed a new way of thinking. Some of the core challenges we faced:
	•	Ground truth limitations: Many QA pairs had only one reference, limiting diversity in acceptable responses.
	•	Nuanced correctness: There was no easy way to distinguish between full correctness, partial correctness, and correct-but-over-informative responses.
	•	Hallucination detection: There was no clean metric to identify when answers contained confident but incorrect additions.

⸻

4. The Breakthrough — LLM-as-a-Judge with Hybrid Verification

“We decided to build the evaluator we wish we had.”

So we built a LLM-as-a-Judge evaluation pipeline, augmented with a deterministic semantic similarity layer. Here’s what that looked like:

🔍 Step 1: Deterministic Similarity Search

We began by using:
	•	BERTScore to judge semantic overlap
	•	Entailment models to measure logical correctness (NLI-based)
	•	ROUGE/METEOR to capture surface similarity

This gave us a scoring scaffold — a baseline for how close the answer was to the reference.

🧠 Step 2: LLM-as-a-Judge

Then we passed all candidate answers to an LLM-based classifier, asking:
	•	Is the answer fully correct, partially correct, incorrect, or correct but with extra factual information?
	•	Is there any hallucination or contradiction?

The LLM considered both:
	•	The ground truth
	•	The user question
	•	The predicted answer

🧪 Result:
	•	Each QA pair was annotated with interpretability and semantic awareness
	•	We could distinguish hallucinations from informative elaborations
	•	We moved from black-box metrics to diagnostic labels

⸻

5. Why Hybrid Wins — The Best of Both Worlds

“Automation plus judgment: deterministic for consistency, LLMs for nuance.”

The hybrid approach:
	•	Avoids over-reliance on LLM variability — the deterministic layer grounds the score.
	•	Brings nuance to correctness — the LLM layer provides human-like judgment on meaning and hallucination.
	•	Detects subtle error types — like confident misstatements or partially correct completions.
	•	Produces structured output — with explainable categories that teams can action on.

⸻

6. Outcome & Impact
	•	We were able to rank and score QA systems not just on “hit or miss” but on semantic quality and factual grounding.
	•	Helped stakeholders understand where a system was better at recall, where it hallucinated, or where it missed important info.
	•	In internal testing, our judge pipeline aligned with human annotators ~92% of the time, far above BLEU and ROUGE (~60–70%).

⸻

7. Future Vision

“Evaluation shouldn’t lag behind model progress. It should lead it.”

This approach sets the foundation for:
	•	Evaluating multi-answer QA (many truths)
	•	Auditing LLMs for factual consistency
	•	Training better RLHF pipelines based on richer feedback signals

⸻

📌 TL;DR for Presentation
	•	Problem: Traditional metrics fail to capture semantic meaning or hallucinations in QA.
	•	Challenge: Need to evaluate answers on correctness, completeness, and hallucination—not just n-gram overlap.
	•	Solution: Built a hybrid evaluation framework combining deterministic similarity metrics + LLM-as-a-Judge.
	•	Impact: Enabled nuanced, explainable, and accurate comparison of QA systems; surfaced insights standard metrics missed.
	•	Why It Matters: Moves us closer to human-aligned, trustworthy QA evaluation.
