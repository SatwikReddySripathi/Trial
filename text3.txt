this is the original


Title: â€œFrom Hallucinations to Correctness: Redefining QA Evaluation with LLM-as-a-Judgeâ€

â¸»

1. Setting the Stage â€” Why This Problem Exists

â€œIn any real-world deployment of QA systems, especially in domains like enterprise search or customer support, correctness isnâ€™t just a luxuryâ€”itâ€™s a business necessity.â€

We were tasked with evaluating an external QA tool against our internal QA system. The goal wasnâ€™t just to compare performance superficially, but to understand how each system handled truth, nuance, and ambiguity â€” not just whether they answered, but how well they answered, and how truthfully.

The traditional approach? Rely on BLEU, ROUGE, or exact match metrics. But hereâ€™s where the real problem started.

â¸»

2. The Problem â€” Standard Metrics Are Blind to Semantics and Hallucinations

â€œThe real world doesnâ€™t speak in exact n-grams.â€

We noticed early on that:
	â€¢	A semantically accurate answer could be penalized if it didnâ€™t match the ground truth textually.
	â€¢	Hallucinated answers, ones that sound plausible but fabricate information, could slip by undetected.
	â€¢	Partially correct or over-informative answers (that go beyond the question but stay factually grounded) had no fair way of being recognized.

This makes traditional evaluation:
	â€¢	Insensitive to meaning
	â€¢	Unable to detect hallucinations
	â€¢	Incapable of rewarding completeness or penalizing unnecessary additions

â¸»

3. Challenges in Redefining Evaluation

We didnâ€™t just need a new metricâ€”we needed a new way of thinking. Some of the core challenges we faced:
	â€¢	Ground truth limitations: Many QA pairs had only one reference, limiting diversity in acceptable responses.
	â€¢	Nuanced correctness: There was no easy way to distinguish between full correctness, partial correctness, and correct-but-over-informative responses.
	â€¢	Hallucination detection: There was no clean metric to identify when answers contained confident but incorrect additions.

â¸»

4. The Breakthrough â€” LLM-as-a-Judge with Hybrid Verification

â€œWe decided to build the evaluator we wish we had.â€

So we built a LLM-as-a-Judge evaluation pipeline, augmented with a deterministic semantic similarity layer. Hereâ€™s what that looked like:

ğŸ” Step 1: Deterministic Similarity Search

We began by using:
	â€¢	BERTScore to judge semantic overlap
	â€¢	Entailment models to measure logical correctness (NLI-based)
	â€¢	ROUGE/METEOR to capture surface similarity

This gave us a scoring scaffold â€” a baseline for how close the answer was to the reference.

ğŸ§  Step 2: LLM-as-a-Judge

Then we passed all candidate answers to an LLM-based classifier, asking:
	â€¢	Is the answer fully correct, partially correct, incorrect, or correct but with extra factual information?
	â€¢	Is there any hallucination or contradiction?

The LLM considered both:
	â€¢	The ground truth
	â€¢	The user question
	â€¢	The predicted answer

ğŸ§ª Result:
	â€¢	Each QA pair was annotated with interpretability and semantic awareness
	â€¢	We could distinguish hallucinations from informative elaborations
	â€¢	We moved from black-box metrics to diagnostic labels

â¸»

5. Why Hybrid Wins â€” The Best of Both Worlds

â€œAutomation plus judgment: deterministic for consistency, LLMs for nuance.â€

The hybrid approach:
	â€¢	Avoids over-reliance on LLM variability â€” the deterministic layer grounds the score.
	â€¢	Brings nuance to correctness â€” the LLM layer provides human-like judgment on meaning and hallucination.
	â€¢	Detects subtle error types â€” like confident misstatements or partially correct completions.
	â€¢	Produces structured output â€” with explainable categories that teams can action on.

â¸»

6. Outcome & Impact
	â€¢	We were able to rank and score QA systems not just on â€œhit or missâ€ but on semantic quality and factual grounding.
	â€¢	Helped stakeholders understand where a system was better at recall, where it hallucinated, or where it missed important info.
	â€¢	In internal testing, our judge pipeline aligned with human annotators ~92% of the time, far above BLEU and ROUGE (~60â€“70%).

â¸»

7. Future Vision

â€œEvaluation shouldnâ€™t lag behind model progress. It should lead it.â€

This approach sets the foundation for:
	â€¢	Evaluating multi-answer QA (many truths)
	â€¢	Auditing LLMs for factual consistency
	â€¢	Training better RLHF pipelines based on richer feedback signals

â¸»

ğŸ“Œ TL;DR for Presentation
	â€¢	Problem: Traditional metrics fail to capture semantic meaning or hallucinations in QA.
	â€¢	Challenge: Need to evaluate answers on correctness, completeness, and hallucinationâ€”not just n-gram overlap.
	â€¢	Solution: Built a hybrid evaluation framework combining deterministic similarity metrics + LLM-as-a-Judge.
	â€¢	Impact: Enabled nuanced, explainable, and accurate comparison of QA systems; surfaced insights standard metrics missed.
	â€¢	Why It Matters: Moves us closer to human-aligned, trustworthy QA evaluation.
