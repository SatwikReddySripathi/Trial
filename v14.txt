"""
Working Hallucination Detection System - Final Version
=====================================================
Complete implementation with all metrics and proper visualization
"""

import numpy as np
from typing import List, Dict, Tuple, Any, Optional
import re
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from collections import defaultdict, Counter
import warnings
import webbrowser
import os
import json
import math

warnings.filterwarnings('ignore')

# Download required NLTK data
for resource in ['punkt', 'stopwords', 'averaged_perceptron_tagger']:
    try:
        nltk.data.find(f'tokenizers/{resource}')
    except LookupError:
        print(f"Downloading {resource}...")
        nltk.download(resource, quiet=True)


class HallucinationDetector:
    """Comprehensive hallucination detection system"""
    
    def __init__(self):
        """Initialize the detector"""
        print("Initializing Hallucination Detector...")
        
        # TF-IDF vectorizer
        self.tfidf = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 3),
            stop_words='english'
        )
        
        self.stopwords = set(stopwords.words('english'))
        
        # Entity patterns
        self.patterns = {
            'money': re.compile(r'\$[\d,]+\.?\d*\s*(?:million|billion|thousand|M|B|K)?', re.I),
            'percentage': re.compile(r'\b\d+\.?\d*\s*(?:%|percent)', re.I),
            'date': re.compile(r'\b(?:Q[1-4]\s*\d{4}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s*\d{1,2},?\s*\d{4}|\d{1,2}[-/]\d{1,2}[-/]\d{2,4})', re.I),
            'number': re.compile(r'\b\d+(?:,\d{3})*(?:\.\d+)?\b')
        }
        
        # Thresholds
        self.semantic_threshold = 0.3  # Minimum similarity for edge creation
        
        print("Initialization complete!")
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """Extract entities from text"""
        entities = {}
        for entity_type, pattern in self.patterns.items():
            entities[entity_type] = pattern.findall(text)
        return entities
    
    def normalize_value(self, value: str, entity_type: str) -> Any:
        """Normalize entity values"""
        try:
            if entity_type == 'money':
                clean = re.sub(r'[$,]', '', value)
                num = float(re.search(r'[\d.]+', clean).group())
                if 'million' in value.lower() or 'M' in value:
                    num *= 1_000_000
                elif 'billion' in value.lower() or 'B' in value:
                    num *= 1_000_000_000
                elif 'thousand' in value.lower() or 'K' in value:
                    num *= 1_000
                return num
            elif entity_type == 'percentage':
                return float(re.search(r'[\d.]+', value).group())
            elif entity_type == 'date':
                return value.lower().replace(',', '').replace('.', '')
            elif entity_type == 'number':
                return float(value.replace(',', ''))
        except:
            return value
        return value
    
    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """Calculate semantic similarity"""
        if not text1 or not text2:
            return 0.0
        
        try:
            # Word overlap for short texts
            words1 = set(word_tokenize(text1.lower())) - self.stopwords
            words2 = set(word_tokenize(text2.lower())) - self.stopwords
            
            if len(words1) < 5 or len(words2) < 5:
                if not words1 or not words2:
                    return 0.0
                return len(words1 & words2) / len(words1 | words2)
            
            # TF-IDF for longer texts
            tfidf_matrix = self.tfidf.fit_transform([text1, text2])
            return float(cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0])
        except:
            return 0.0
    
    def calculate_entailment(self, premise: str, hypothesis: str) -> Dict[str, float]:
        """Simple entailment calculation"""
        semantic_sim = self.calculate_semantic_similarity(premise, hypothesis)
        
        # Check for negations
        negations = {'not', 'no', 'never', 'neither', 'none'}
        premise_has_neg = bool(set(word_tokenize(premise.lower())) & negations)
        hypo_has_neg = bool(set(word_tokenize(hypothesis.lower())) & negations)
        
        # Simple scoring
        if premise_has_neg != hypo_has_neg and semantic_sim > 0.5:
            # Contradiction
            return {
                'entailment': 0.1,
                'contradiction': 0.8,
                'neutral': 0.1
            }
        elif semantic_sim > 0.7:
            # High similarity = entailment
            return {
                'entailment': semantic_sim,
                'contradiction': 0.1,
                'neutral': 1 - semantic_sim - 0.1
            }
        else:
            # Neutral
            return {
                'entailment': semantic_sim * 0.5,
                'contradiction': 0.1,
                'neutral': 0.9 - semantic_sim * 0.5
            }
    
    def calculate_factual_consistency(self, ref_entities: Dict, cand_entities: Dict) -> float:
        """Calculate factual consistency"""
        total_matches = 0
        total_entities = 0
        
        for entity_type in ref_entities:
            ref_norm = set()
            cand_norm = set()
            
            # Normalize entities
            for entity in ref_entities[entity_type]:
                ref_norm.add(str(self.normalize_value(entity, entity_type)))
            
            for entity in cand_entities[entity_type]:
                cand_norm.add(str(self.normalize_value(entity, entity_type)))
            
            if ref_norm or cand_norm:
                matches = len(ref_norm & cand_norm)
                total = len(ref_norm | cand_norm)
                total_matches += matches
                total_entities += total
        
        return total_matches / total_entities if total_entities > 0 else 1.0
    
    def calculate_entropy(self, text: str) -> float:
        """Calculate text entropy"""
        words = [w.lower() for w in word_tokenize(text) if w.isalnum() and w not in self.stopwords]
        
        if not words:
            return 0.0
        
        # Word frequency
        word_freq = Counter(words)
        total = len(words)
        
        # Shannon entropy
        entropy = 0
        for count in word_freq.values():
            p = count / total
            if p > 0:
                entropy -= p * math.log2(p)
        
        # Normalize
        max_entropy = math.log2(len(word_freq)) if len(word_freq) > 1 else 1
        return entropy / max_entropy if max_entropy > 0 else 0
    
    def analyze_paragraph(self, reference: str, candidate: str, idx: int) -> Dict[str, Any]:
        """Analyze a single paragraph"""
        # Extract entities
        ref_entities = self.extract_entities(reference)
        cand_entities = self.extract_entities(candidate)
        
        # Calculate metrics
        semantic_sim = self.calculate_semantic_similarity(reference, candidate)
        entailment = self.calculate_entailment(reference, candidate)
        factual_consistency = self.calculate_factual_consistency(ref_entities, cand_entities)
        
        # Entropy
        ref_entropy = self.calculate_entropy(reference)
        cand_entropy = self.calculate_entropy(candidate)
        entropy_div = abs(cand_entropy - ref_entropy) / (ref_entropy + 0.001)
        
        # Consistency breakdown
        consistency_scores = {
            'semantic': semantic_sim,
            'factual': factual_consistency,
            'entailment': entailment['entailment'],
            'non_contradiction': 1.0 - entailment['contradiction']
        }
        
        # Overall consistency
        overall_consistency = sum(consistency_scores.values()) / len(consistency_scores)
        
        # Hallucination score
        hall_score = 0.0
        hall_score += (1 - semantic_sim) * 0.3
        hall_score += (1 - factual_consistency) * 0.3
        hall_score += entailment['contradiction'] * 0.3
        hall_score += min(entropy_div * 0.1, 0.1)
        
        # Determine severity
        if hall_score > 0.7:
            severity = 'severe'
        elif hall_score > 0.5:
            severity = 'moderate'
        elif hall_score > 0.3:
            severity = 'minor'
        else:
            severity = 'none'
        
        return {
            'id': idx,
            'text': candidate,
            'is_hallucinated': hall_score > 0.5,
            'hallucination_score': float(hall_score),
            'severity': severity,
            'semantic_similarity': float(semantic_sim),
            'entailment_score': float(entailment['entailment']),
            'contradiction_score': float(entailment['contradiction']),
            'factual_consistency': float(factual_consistency),
            'entropy': float(cand_entropy),
            'entropy_divergence': float(entropy_div),
            'consistency_breakdown': consistency_scores,
            'overall_consistency': float(overall_consistency)
        }
    
    def build_graph(self, reference: str, analyses: List[Dict]) -> nx.Graph:
        """Build graph with coherent connections"""
        G = nx.Graph()
        
        # Add nodes
        # Reference node
        G.add_node(0)
        G.nodes[0]['label'] = 'REF'
        G.nodes[0]['text'] = reference[:200] + '...' if len(reference) > 200 else reference
        G.nodes[0]['type'] = 'reference'
        G.nodes[0]['severity'] = 'reference'
        
        # Candidate nodes
        for analysis in analyses:
            node_id = analysis['id']
            G.add_node(node_id)
            G.nodes[node_id]['label'] = f'P{node_id}'
            G.nodes[node_id]['text'] = analysis['text'][:200] + '...' if len(analysis['text']) > 200 else analysis['text']
            G.nodes[node_id]['type'] = 'candidate'
            G.nodes[node_id]['severity'] = analysis['severity']
            G.nodes[node_id]['hallucinated'] = analysis['is_hallucinated']
            G.nodes[node_id]['scores'] = {
                'hallucination': analysis['hallucination_score'],
                'semantic': analysis['semantic_similarity'],
                'entailment': analysis['entailment_score'],
                'contradiction': analysis['contradiction_score'],
                'factual': analysis['factual_consistency'],
                'entropy': analysis['entropy'],
                'consistency': analysis['overall_consistency']
            }
        
        # Add edges only for semantically coherent pairs
        all_texts = [reference] + [a['text'] for a in analyses]
        
        for i in range(len(all_texts)):
            for j in range(i + 1, len(all_texts)):
                similarity = self.calculate_semantic_similarity(all_texts[i], all_texts[j])
                
                if similarity >= self.semantic_threshold:
                    G.add_edge(i, j, weight=float(similarity))
        
        return G
    
    def create_visualization(self, G: nx.Graph, analyses: List[Dict], 
                           output_path: str = "hallucination_detection.html") -> str:
        """Create visualization"""
        # Prepare data for JSON
        nodes = []
        for node_id in G.nodes():
            node_data = G.nodes[node_id]
            
            # Color based on severity
            colors = {
                'reference': '#0066CC',
                'severe': '#CC0000',
                'moderate': '#FF6666', 
                'minor': '#FFAA00',
                'none': '#00CC00'
            }
            
            nodes.append({
                'id': int(node_id),
                'label': node_data['label'],
                'color': colors.get(node_data['severity'], '#808080'),
                'size': 25 if node_data['type'] == 'reference' else 20,
                'text': node_data['text'],
                'type': node_data['type'],
                'severity': node_data['severity'],
                'hallucinated': node_data.get('hallucinated', False),
                'scores': node_data.get('scores', {})
            })
        
        edges = []
        for u, v, data in G.edges(data=True):
            edges.append({
                'source': int(u),
                'target': int(v),
                'weight': float(data['weight'])
            })
        
        # Calculate summary stats
        total_analyses = len(analyses)
        hallucinated = sum(1 for a in analyses if a['is_hallucinated'])
        severe = sum(1 for a in analyses if a['severity'] == 'severe')
        moderate = sum(1 for a in analyses if a['severity'] == 'moderate')
        minor = sum(1 for a in analyses if a['severity'] == 'minor')
        avg_consistency = sum(a['overall_consistency'] for a in analyses) / len(analyses) if analyses else 0
        
        # HTML template
        html = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Hallucination Detection Results</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <style>
        body {{
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background: #f5f5f5;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
        }}
        h1 {{
            text-align: center;
            color: #333;
        }}
        .summary {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 15px;
            margin: 30px 0;
        }}
        .stat-card {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .stat-value {{
            font-size: 32px;
            font-weight: bold;
            color: #0066CC;
        }}
        .stat-label {{
            color: #666;
            margin-top: 5px;
        }}
        #graph {{
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin: 30px 0;
        }}
        .tooltip {{
            position: absolute;
            padding: 12px;
            background: rgba(0, 0, 0, 0.9);
            color: white;
            border-radius: 8px;
            pointer-events: none;
            opacity: 0;
            font-size: 13px;
            max-width: 400px;
        }}
        .legend {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin-bottom: 20px;
        }}
        .legend-item {{
            display: inline-block;
            margin-right: 20px;
            margin-bottom: 10px;
        }}
        .legend-color {{
            display: inline-block;
            width: 20px;
            height: 20px;
            border-radius: 50%;
            vertical-align: middle;
            margin-right: 8px;
        }}
        .controls {{
            text-align: center;
            margin: 20px 0;
        }}
        button {{
            padding: 10px 20px;
            margin: 0 5px;
            border: none;
            border-radius: 5px;
            background: #0066CC;
            color: white;
            cursor: pointer;
        }}
        button:hover {{
            background: #0052A3;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>Hallucination Detection Analysis</h1>
        
        <div class="summary">
            <div class="stat-card">
                <div class="stat-value">{total_analyses}</div>
                <div class="stat-label">Total Analyzed</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{hallucinated}</div>
                <div class="stat-label">Hallucinated</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{severe}</div>
                <div class="stat-label">Severe</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{moderate}</div>
                <div class="stat-label">Moderate</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{minor}</div>
                <div class="stat-label">Minor</div>
            </div>
            <div class="stat-card">
                <div class="stat-value">{avg_consistency:.1%}</div>
                <div class="stat-label">Avg Consistency</div>
            </div>
        </div>
        
        <div class="legend">
            <h3>Legend</h3>
            <div class="legend-item">
                <span class="legend-color" style="background: #0066CC;"></span>
                Reference
            </div>
            <div class="legend-item">
                <span class="legend-color" style="background: #CC0000;"></span>
                Severe
            </div>
            <div class="legend-item">
                <span class="legend-color" style="background: #FF6666;"></span>
                Moderate
            </div>
            <div class="legend-item">
                <span class="legend-color" style="background: #FFAA00;"></span>
                Minor
            </div>
            <div class="legend-item">
                <span class="legend-color" style="background: #00CC00;"></span>
                None
            </div>
        </div>
        
        <div class="controls">
            <button onclick="resetView()">Reset View</button>
            <button onclick="toggleLabels()">Toggle Labels</button>
        </div>
        
        <div id="graph"></div>
        <div class="tooltip"></div>
        
        <p style="text-align: center; color: #666; margin-top: 20px;">
            Edges show semantic similarity ≥ {self.semantic_threshold:.0%}. 
            Thicker edges indicate stronger similarity.
        </p>
    </div>
    
    <script>
        const nodes = {json.dumps(nodes)};
        const links = {json.dumps(edges)};
        
        const width = 1200;
        const height = 600;
        
        const svg = d3.select("#graph")
            .append("svg")
            .attr("width", width)
            .attr("height", height);
        
        const g = svg.append("g");
        
        const zoom = d3.zoom()
            .scaleExtent([0.1, 10])
            .on("zoom", (event) => {{
                g.attr("transform", event.transform);
            }});
        
        svg.call(zoom);
        
        const simulation = d3.forceSimulation(nodes)
            .force("link", d3.forceLink(links).id(d => d.id).distance(150))
            .force("charge", d3.forceManyBody().strength(-400))
            .force("center", d3.forceCenter(width / 2, height / 2));
        
        const link = g.append("g")
            .selectAll("line")
            .data(links)
            .enter().append("line")
            .attr("stroke", "#999")
            .attr("stroke-opacity", 0.6)
            .attr("stroke-width", d => 1 + d.weight * 5);
        
        const node = g.append("g")
            .selectAll("circle")
            .data(nodes)
            .enter().append("circle")
            .attr("r", d => d.size)
            .attr("fill", d => d.color)
            .attr("stroke", "#333")
            .attr("stroke-width", 2)
            .style("cursor", "pointer")
            .call(d3.drag()
                .on("start", dragstarted)
                .on("drag", dragged)
                .on("end", dragended));
        
        const labels = g.append("g")
            .selectAll("text")
            .data(nodes)
            .enter().append("text")
            .text(d => d.label)
            .attr("font-size", 14)
            .attr("font-weight", "bold")
            .attr("text-anchor", "middle")
            .attr("dy", -30);
        
        const tooltip = d3.select(".tooltip");
        
        node.on("mouseover", function(event, d) {{
            let html = `<strong>${{d.label}}</strong><br/>`;
            html += `Type: ${{d.type}}<br/>`;
            
            if (d.type === 'candidate') {{
                html += `Severity: ${{d.severity}}<br/>`;
                html += `Hallucinated: ${{d.hallucinated ? 'Yes' : 'No'}}<br/><br/>`;
                
                html += `<strong>Scores:</strong><br/>`;
                html += `Hallucination: ${{(d.scores.hallucination * 100).toFixed(1)}}%<br/>`;
                html += `Consistency: ${{(d.scores.consistency * 100).toFixed(1)}}%<br/>`;
                html += `Semantic Sim: ${{(d.scores.semantic * 100).toFixed(1)}}%<br/>`;
                html += `Factual: ${{(d.scores.factual * 100).toFixed(1)}}%<br/>`;
                html += `Entailment: ${{(d.scores.entailment * 100).toFixed(1)}}%<br/>`;
                html += `Contradiction: ${{(d.scores.contradiction * 100).toFixed(1)}}%<br/>`;
                html += `Entropy: ${{d.scores.entropy.toFixed(3)}}<br/>`;
            }}
            
            html += `<br/><strong>Text:</strong><br/>${{d.text}}`;
            
            tooltip.html(html)
                .style("left", (event.pageX + 10) + "px")
                .style("top", (event.pageY - 10) + "px")
                .style("opacity", 1);
        }})
        .on("mouseout", function() {{
            tooltip.style("opacity", 0);
        }});
        
        simulation.on("tick", () => {{
            link
                .attr("x1", d => d.source.x)
                .attr("y1", d => d.source.y)
                .attr("x2", d => d.target.x)
                .attr("y2", d => d.target.y);
            
            node
                .attr("cx", d => d.x)
                .attr("cy", d => d.y);
            
            labels
                .attr("x", d => d.x)
                .attr("y", d => d.y);
        }});
        
        function dragstarted(event, d) {{
            if (!event.active) simulation.alphaTarget(0.3).restart();
            d.fx = d.x;
            d.fy = d.y;
        }}
        
        function dragged(event, d) {{
            d.fx = event.x;
            d.fy = event.y;
        }}
        
        function dragended(event, d) {{
            if (!event.active) simulation.alphaTarget(0);
            d.fx = null;
            d.fy = null;
        }}
        
        function resetView() {{
            svg.transition().duration(750).call(
                zoom.transform,
                d3.zoomIdentity
            );
        }}
        
        let labelsVisible = true;
        function toggleLabels() {{
            labelsVisible = !labelsVisible;
            labels.style("display", labelsVisible ? "block" : "none");
        }}
    </script>
</body>
</html>
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html)
        
        return output_path
    
    def analyze_file(self, reference: str, file_path: str, output_dir: str = ".") -> Dict[str, Any]:
        """Analyze candidates from file"""
        print(f"\nReading from: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Split by double newlines
            candidates = [p.strip() for p in content.split('\n\n') if p.strip()]
            print(f"Found {len(candidates)} paragraphs")
            
            return self.analyze(reference, candidates, output_dir)
        
        except Exception as e:
            print(f"Error reading file: {e}")
            return {}
    
    def analyze(self, reference: str, candidates: List[str], output_dir: str = ".") -> Dict[str, Any]:
        """Main analysis function"""
        print("\n" + "="*60)
        print("HALLUCINATION DETECTION ANALYSIS")
        print("="*60)
        
        # Analyze each candidate
        analyses = []
        for i, candidate in enumerate(candidates, 1):
            print(f"\nAnalyzing paragraph {i}...")
            analysis = self.analyze_paragraph(reference, candidate, i)
            analyses.append(analysis)
            
            print(f"  Severity: {analysis['severity']}")
            print(f"  Consistency: {analysis['overall_consistency']:.1%}")
            print(f"  Hallucination Score: {analysis['hallucination_score']:.1%}")
        
        # Build graph
        print("\nBuilding graph...")
        G = self.build_graph(reference, analyses)
        print(f"Created {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")
        
        # Create visualization
        print("Creating visualization...")
        viz_path = os.path.join(output_dir, "hallucination_analysis.html")
        self.create_visualization(G, analyses, viz_path)
        
        # Summary
        total_consistency = sum(a['overall_consistency'] for a in analyses) / len(analyses)
        
        print("\n" + "-"*60)
        print("SUMMARY")
        print("-"*60)
        print(f"Total analyzed: {len(candidates)}")
        print(f"Average consistency: {total_consistency:.1%}")
        print(f"Hallucinated: {sum(1 for a in analyses if a['is_hallucinated'])}")
        
        print(f"\nVisualization saved to: {viz_path}")
        print("Opening in browser...")
        
        try:
            webbrowser.open(f'file://{os.path.abspath(viz_path)}')
        except:
            print("Please open the file manually")
        
        return {
            'analyses': analyses,
            'graph': G,
            'visualization': viz_path,
            'total_consistency': total_consistency
        }


# Example usage
if __name__ == "__main__":
    # Initialize detector
    detector = HallucinationDetector()
    
    # Reference text
    reference = """The company reported revenue of $2.5 million in Q4 2023, with a 15% increase from the previous quarter. 
    CEO John Smith announced expansion plans on January 15, 2024, targeting the Asian market. 
    The profit margin improved to 22% due to cost optimization strategies."""
    
    # Test candidates
    candidates = [
        """Q4 2023 revenue reached $2.5 million, marking 15% quarterly growth. 
        John Smith revealed Asian market expansion on January 15, 2024. 
        Profit margins rose to 22% through cost optimization.""",
        
        """The company earned $3.2 million in Q4 2023, a 20% increase. 
        CEO John Smith discussed expansion plans on January 20, 2024. 
        Profit margins reached 25% after cost controls.""",
        
        """In Q4 2023, revenue of $2.5 million represented 15% growth. 
        The Asian market expansion was announced by CEO John Smith. 
        Cost optimization led to 22% profit margins.""",
        
        """Business performance showed positive trends in late 2023. 
        International expansion remains a key strategic priority. 
        Financial metrics continue to improve significantly."""
    ]
    
    # Run analysis
    results = detector.analyze(reference, candidates)
    
    # Test file reading
    print("\n\nTesting file reading...")
    with open("test_candidates.txt", 'w') as f:
        f.write('\n\n'.join(candidates))
    
    file_results = detector.analyze_file(reference, "test_candidates.txt")
