import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Set
import re
from datetime import datetime
import networkx as nx
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import spacy
from collections import defaultdict
import seaborn as sns

class HallucinationDetector:
    def __init__(self):
        # Initialize models
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        # Use correct model name for NLI
        self.nli_model = pipeline("text-classification", model="cross-encoder/nli-deberta-v3-base")
        self.nlp = spacy.load("en_core_web_sm")
        
        # Patterns for extraction
        self.date_pattern = r'\b(?:\d{1,2}[-/]\d{1,2}[-/]\d{2,4}|\d{4}[-/]\d{1,2}[-/]\d{1,2}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4})\b'
        self.money_pattern = r'\$[\d,]+(?:\.\d{2})?|\b\d+(?:,\d{3})*(?:\.\d{2})?\s*(?:dollars?|USD|cents?)\b'
        self.number_pattern = r'\b\d+(?:,\d{3})*(?:\.\d+)?\b'
        
    def extract_entities(self, text: str) -> Dict[str, Set]:
        """Extract important information from text"""
        entities = {
            'dates': set(re.findall(self.date_pattern, text, re.IGNORECASE)),
            'money': set(re.findall(self.money_pattern, text, re.IGNORECASE)),
            'numbers': set(re.findall(self.number_pattern, text)),
            'named_entities': set(),
            'facts': []
        }
        
        # Extract named entities using spaCy
        doc = self.nlp(text)
        for ent in doc.ents:
            if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT']:
                entities['named_entities'].add((ent.text, ent.label_))
        
        # Extract key facts (sentences with entities)
        sentences = text.split('.')
        for sent in sentences:
            if any(ent in sent for ent_list in [entities['dates'], entities['money']] for ent in ent_list):
                entities['facts'].append(sent.strip())
        
        return entities
    
    def semantic_similarity(self, text1: str, text2: str) -> float:
        """Calculate semantic similarity between two texts"""
        embeddings = self.sentence_model.encode([text1, text2])
        similarity = np.dot(embeddings[0], embeddings[1]) / (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1]))
        return float(similarity)
    
    def check_entailment(self, premise: str, hypothesis: str) -> Dict[str, float]:
        """Check if premise entails hypothesis using NLI model"""
        # Format input for the model
        input_text = f"{premise} [SEP] {hypothesis}"
        result = self.nli_model(input_text)
        
        # Convert to entailment scores
        entailment_scores = {
            'entailment': 0.0,
            'neutral': 0.0,
            'contradiction': 0.0
        }
        
        # Map model outputs to standard labels
        label_mapping = {
            'ENTAILMENT': 'entailment',
            'NEUTRAL': 'neutral',
            'CONTRADICTION': 'contradiction'
        }
        
        for item in result:
            label = item['label'].upper()
            if label in label_mapping:
                entailment_scores[label_mapping[label]] = item['score']
        
        return entailment_scores
    
    def compare_entities(self, entities1: Dict, entities2: Dict) -> float:
        """Compare extracted entities between two texts"""
        consistency_scores = []
        
        # Compare dates
        if entities1['dates'] and entities2['dates']:
            date_overlap = len(entities1['dates'] & entities2['dates']) / max(len(entities1['dates']), len(entities2['dates']))
            consistency_scores.append(date_overlap)
        
        # Compare money values
        if entities1['money'] and entities2['money']:
            money_overlap = len(entities1['money'] & entities2['money']) / max(len(entities1['money']), len(entities2['money']))
            consistency_scores.append(money_overlap)
        
        # Compare named entities
        if entities1['named_entities'] and entities2['named_entities']:
            ne_overlap = len(entities1['named_entities'] & entities2['named_entities']) / max(len(entities1['named_entities']), len(entities2['named_entities']))
            consistency_scores.append(ne_overlap)
        
        return np.mean(consistency_scores) if consistency_scores else 1.0
    
    def classify_hallucination(self, reference_text: str, candidate_text: str) -> Dict:
        """Classify if candidate text contains hallucinations compared to reference"""
        # Extract entities
        ref_entities = self.extract_entities(reference_text)
        cand_entities = self.extract_entities(candidate_text)
        
        # Calculate metrics
        semantic_sim = self.semantic_similarity(reference_text, candidate_text)
        entity_consistency = self.compare_entities(ref_entities, cand_entities)
        entailment_scores = self.check_entailment(reference_text, candidate_text)
        
        # Combine scores for final classification
        hallucination_score = (
            (1 - semantic_sim) * 0.3 +
            (1 - entity_consistency) * 0.4 +
            entailment_scores['contradiction'] * 0.3
        )
        
        is_hallucinated = hallucination_score > 0.5
        
        return {
            'is_hallucinated': is_hallucinated,
            'hallucination_score': hallucination_score,
            'semantic_similarity': semantic_sim,
            'entity_consistency': entity_consistency,
            'entailment_scores': entailment_scores,
            'extracted_entities': {
                'reference': ref_entities,
                'candidate': cand_entities
            }
        }
    
    def build_consistency_graph(self, paragraphs: List[str], reference_idx: int = 0) -> nx.DiGraph:
        """Build a directed graph of paragraph consistencies"""
        G = nx.DiGraph()
        n = len(paragraphs)
        
        # Add nodes
        for i, para in enumerate(paragraphs):
            G.add_node(i, text=para[:50] + "...", is_reference=(i == reference_idx))
        
        # Add edges with consistency scores
        consistency_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                if i != j:
                    result = self.classify_hallucination(paragraphs[i], paragraphs[j])
                    consistency = 1 - result['hallucination_score']
                    consistency_matrix[i][j] = consistency
                    
                    # Add edge if consistency is above threshold
                    if consistency > 0.5:
                        G.add_edge(i, j, weight=consistency, 
                                 semantic_sim=result['semantic_similarity'],
                                 entity_consistency=result['entity_consistency'])
        
        return G, consistency_matrix
    
    def calculate_graph_metrics(self, G: nx.DiGraph) -> Dict:
        """Calculate graph-based consistency metrics"""
        metrics = {}
        
        # Basic metrics
        metrics['num_nodes'] = G.number_of_nodes()
        metrics['num_edges'] = G.number_of_edges()
        metrics['density'] = nx.density(G)
        
        # Centrality measures
        metrics['in_degree_centrality'] = nx.in_degree_centrality(G)
        metrics['out_degree_centrality'] = nx.out_degree_centrality(G)
        metrics['pagerank'] = nx.pagerank(G, weight='weight')
        
        # Clustering and connectivity
        undirected_G = G.to_undirected()
        metrics['clustering_coefficient'] = nx.average_clustering(undirected_G, weight='weight')
        metrics['is_strongly_connected'] = nx.is_strongly_connected(G)
        
        # Calculate overall consistency score
        if G.number_of_edges() > 0:
            edge_weights = [G[u][v]['weight'] for u, v in G.edges()]
            metrics['average_edge_weight'] = np.mean(edge_weights)
            metrics['consistency_variance'] = np.var(edge_weights)
        else:
            metrics['average_edge_weight'] = 0
            metrics['consistency_variance'] = 0
        
        # Final consistency score (weighted combination)
        metrics['final_consistency_score'] = (
            metrics['average_edge_weight'] * 0.4 +
            metrics['density'] * 0.2 +
            metrics['clustering_coefficient'] * 0.2 +
            (1 - metrics['consistency_variance']) * 0.2
        )
        
        return metrics
    
    def visualize_graph(self, G: nx.DiGraph, consistency_matrix: np.ndarray, 
                       save_path: str = "hallucination_graph.png"):
        """Visualize the consistency graph"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
        
        # Graph visualization
        pos = nx.spring_layout(G, k=2, iterations=50)
        
        # Node colors based on PageRank
        pagerank = nx.pagerank(G, weight='weight')
        node_colors = [pagerank[node] for node in G.nodes()]
        
        # Edge colors based on weights
        edges = G.edges()
        weights = [G[u][v]['weight'] for u, v in edges]
        
        # Draw graph
        nx.draw_networkx_nodes(G, pos, node_color=node_colors, 
                              cmap='RdYlGn', node_size=1000, ax=ax1)
        nx.draw_networkx_labels(G, pos, ax=ax1)
        nx.draw_networkx_edges(G, pos, edge_color=weights, edge_cmap=plt.cm.Blues, 
                              width=2, alpha=0.6, ax=ax1)
        
        # Add colorbar for edges
        sm = plt.cm.ScalarMappable(cmap=plt.cm.Blues, 
                                   norm=plt.Normalize(vmin=min(weights), vmax=max(weights)))
        sm.set_array([])
        cbar = plt.colorbar(sm, ax=ax1)
        cbar.set_label('Consistency Score', rotation=270, labelpad=20)
        
        ax1.set_title("Paragraph Consistency Graph")
        ax1.axis('off')
        
        # Heatmap visualization
        sns.heatmap(consistency_matrix, annot=True, fmt='.2f', cmap='RdYlGn', 
                   square=True, ax=ax2, cbar_kws={'label': 'Consistency'})
        ax2.set_title("Pairwise Consistency Matrix")
        ax2.set_xlabel("Target Paragraph")
        ax2.set_ylabel("Source Paragraph")
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        return fig
    
    def analyze_paragraphs(self, reference_paragraph: str, candidate_paragraphs: List[str], 
                          visualize: bool = True) -> Dict:
        """Main analysis function"""
        # Combine all paragraphs for analysis
        all_paragraphs = [reference_paragraph] + candidate_paragraphs
        
        # Build consistency graph
        G, consistency_matrix = self.build_consistency_graph(all_paragraphs, reference_idx=0)
        
        # Calculate metrics
        graph_metrics = self.calculate_graph_metrics(G)
        
        # Classify each candidate paragraph
        classifications = []
        for i, candidate in enumerate(candidate_paragraphs, 1):
            result = self.classify_hallucination(reference_paragraph, candidate)
            result['paragraph_index'] = i
            classifications.append(result)
        
        # Visualize if requested
        if visualize:
            self.visualize_graph(G, consistency_matrix)
        
        # Compile results
        results = {
            'graph': G,
            'consistency_matrix': consistency_matrix,
            'graph_metrics': graph_metrics,
            'classifications': classifications,
            'summary': {
                'total_paragraphs': len(all_paragraphs),
                'hallucinated_count': sum(1 for c in classifications if c['is_hallucinated']),
                'average_consistency': graph_metrics['average_edge_weight'],
                'final_consistency_score': graph_metrics['final_consistency_score']
            }
        }
        
        return results

# Example usage
if __name__ == "__main__":
    # Initialize detector
    detector = HallucinationDetector()
    
    # Example data
    reference = "The company reported revenue of $2.5 million in Q4 2023, with a 15% increase from the previous quarter. The CEO John Smith announced the expansion plans on January 15, 2024."
    
    candidates = [
        "In Q4 2023, the company's revenue reached $2.5 million, marking a 15% growth compared to Q3. CEO John Smith revealed expansion strategies on January 15, 2024.",
        "The firm generated $3.2 million in revenue during Q4 2023, showing 20% growth. CEO John Smith discussed expansion on January 20, 2024.",
        "Revenue for Q4 2023 was $2.5 million with significant growth. The company plans to expand operations as announced by leadership in early 2024."
    ]
    
    # Analyze
    results = detector.analyze_paragraphs(reference, candidates)
    
    # Print results
    print(f"Final Consistency Score: {results['summary']['final_consistency_score']:.3f}")
    print(f"Hallucinated Paragraphs: {results['summary']['hallucinated_count']}/{len(candidates)}")
    
    for classification in results['classifications']:
        print(f"\nParagraph {classification['paragraph_index']}:")
        print(f"  Hallucinated: {classification['is_hallucinated']}")
        print(f"  Hallucination Score: {classification['hallucination_score']:.3f}")
        print(f"  Semantic Similarity: {classification['semantic_similarity']:.3f}")
