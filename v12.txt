"""
Fool-Proof Hallucination Detection System
=========================================
A more rigorous approach to hallucination detection with multiple verification layers
and better handling of edge cases.
"""

import numpy as np
from typing import List, Dict, Tuple, Set, Optional, Union
import re
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from collections import defaultdict
import warnings
from scipy.stats import entropy
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import torch
import webbrowser
import os
import json
from dataclasses import dataclass, field
from enum import Enum
import dateutil.parser as date_parser
from difflib import SequenceMatcher
import spacy

warnings.filterwarnings('ignore')

# Download required NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('maxent_ne_chunker', quiet=True)
nltk.download('words', quiet=True)


class VerificationStatus(Enum):
    """Verification status for claims"""
    VERIFIED = "verified"  # Directly supported by reference
    CONTRADICTED = "contradicted"  # Directly contradicts reference
    UNSUPPORTED = "unsupported"  # Not mentioned in reference
    INFERRED = "inferred"  # Reasonable inference from reference
    PARTIAL = "partial"  # Partially supported/contradicted


@dataclass
class Fact:
    """Represents a verifiable fact"""
    text: str
    fact_type: str  # 'quantitative', 'temporal', 'relational', 'descriptive'
    subject: Optional[str] = None
    predicate: Optional[str] = None
    object: Optional[str] = None
    value: Optional[Union[str, float]] = None
    normalized_value: Optional[Union[str, float]] = None
    context: str = ""
    confidence: float = 1.0
    source_span: Tuple[int, int] = (0, 0)


@dataclass
class FactVerification:
    """Result of fact verification"""
    fact: Fact
    status: VerificationStatus
    supporting_facts: List[Fact] = field(default_factory=list)
    contradicting_facts: List[Fact] = field(default_factory=list)
    confidence: float = 0.0
    explanation: str = ""


@dataclass
class HallucinationAnalysis:
    """Comprehensive hallucination analysis result"""
    is_hallucinated: bool
    confidence: float
    verification_results: List[FactVerification]
    hallucination_patterns: Dict[str, float]
    graph_metrics: Dict[str, float]
    explanation: str
    severity: str  # 'none', 'minor', 'moderate', 'severe'


class FoolProofHallucinationDetector:
    """Advanced hallucination detection with multiple verification layers"""
    
    def __init__(self, use_gpu=False):
        """Initialize with all required models and resources"""
        self.device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'
        print(f"Initializing Fool-Proof Hallucination Detector on {self.device}...")
        
        # Models
        self.sentence_model = SentenceTransformer('all-mpnet-base-v2')  # Better model
        self.sentence_model.to(self.device)
        
        self.nli_pipeline = pipeline(
            "text-classification",
            model="microsoft/deberta-v3-large-mnli",  # More accurate
            device=0 if self.device == 'cuda' else -1
        )
        
        # Load SpaCy for better NLP
        try:
            self.nlp = spacy.load("en_core_web_sm")
        except:
            print("Installing spacy model...")
            os.system("python -m spacy download en_core_web_sm")
            self.nlp = spacy.load("en_core_web_sm")
        
        # Enhanced patterns with better coverage
        self.fact_patterns = {
            'money': {
                'patterns': [
                    r'\$[\d,]+\.?\d*\s*(?:million|billion|thousand|M|B|K)?',
                    r'[\d,]+\.?\d*\s*(?:dollars|USD|euros|EUR|pounds|GBP)',
                    r'(?:revenue|profit|loss|cost|price|value|worth)\s*(?:of|was|is|reached|hit)?\s*\$?[\d,]+\.?\d*'
                ],
                'normalizer': self._normalize_money
            },
            'percentage': {
                'patterns': [
                    r'\d+\.?\d*\s*(?:%|percent|percentage)',
                    r'(?:increase|decrease|growth|decline|rise|fall)\s*(?:of|by)?\s*\d+\.?\d*\s*(?:%|percent)?',
                    r'\d+\.?\d*\s*(?:%|percent)\s*(?:increase|decrease|growth|decline|rise|fall)'
                ],
                'normalizer': self._normalize_percentage
            },
            'date': {
                'patterns': [
                    r'\b(?:Q[1-4]\s*\d{4})\b',
                    r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\.?\s*\d{1,2},?\s*\d{4}\b',
                    r'\b\d{1,2}[-/]\d{1,2}[-/]\d{2,4}\b',
                    r'\b\d{4}[-/]\d{1,2}[-/]\d{1,2}\b',
                    r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s*\d{1,2},?\s*\d{4}\b'
                ],
                'normalizer': self._normalize_date
            },
            'quantity': {
                'patterns': [
                    r'\b\d+\s*(?:employees?|customers?|users?|units?|items?|products?)\b',
                    r'\b(?:employees?|customers?|users?|units?|items?|products?)\s*(?:of|totaling|numbering)?\s*\d+\b'
                ],
                'normalizer': self._normalize_quantity
            }
        }
        
        # Verification thresholds
        self.thresholds = {
            'semantic_similarity': 0.85,  # Very high for same fact
            'fact_similarity': 0.9,       # Near-identical facts
            'contradiction': 0.8,         # High confidence contradiction
            'inference': 0.6,            # Reasonable inference
            'hallucination': 0.7         # Overall hallucination threshold
        }
        
        print("Initialization complete!")
    
    def extract_facts(self, text: str) -> List[Fact]:
        """Extract all verifiable facts from text"""
        facts = []
        doc = self.nlp(text)
        
        # Extract facts by sentence
        for sent in doc.sents:
            sent_text = sent.text.strip()
            
            # Extract quantitative facts
            for fact_type, config in self.fact_patterns.items():
                for pattern in config['patterns']:
                    matches = re.finditer(pattern, sent_text, re.IGNORECASE)
                    for match in matches:
                        # Get surrounding context
                        start = max(0, match.start() - 50)
                        end = min(len(sent_text), match.end() + 50)
                        context = sent_text[start:end]
                        
                        # Create fact
                        fact = Fact(
                            text=match.group(),
                            fact_type='quantitative',
                            value=match.group(),
                            normalized_value=config['normalizer'](match.group()),
                            context=context,
                            source_span=(sent.start_char + match.start(), 
                                       sent.start_char + match.end())
                        )
                        
                        # Extract subject-predicate-object if possible
                        self._extract_spo(sent, fact)
                        facts.append(fact)
            
            # Extract relational facts (X is Y, X has Y, etc.)
            if any(token.pos_ in ['VERB', 'AUX'] for token in sent):
                # Look for subject-verb-object patterns
                for token in sent:
                    if token.dep_ == "ROOT":
                        subject = None
                        obj = None
                        
                        # Find subject
                        for child in token.children:
                            if child.dep_ in ["nsubj", "nsubjpass"]:
                                subject = child.text
                                # Include compound subjects
                                subject = ' '.join([t.text for t in child.subtree])
                        
                        # Find object
                        for child in token.children:
                            if child.dep_ in ["dobj", "attr", "pobj"]:
                                obj = ' '.join([t.text for t in child.subtree])
                        
                        if subject and obj:
                            fact = Fact(
                                text=sent_text,
                                fact_type='relational',
                                subject=subject,
                                predicate=token.text,
                                object=obj,
                                context=sent_text,
                                source_span=(sent.start_char, sent.end_char)
                            )
                            facts.append(fact)
        
        return facts
    
    def _extract_spo(self, sent, fact: Fact):
        """Extract subject-predicate-object for a fact"""
        # Find the token containing the fact value
        fact_tokens = []
        for token in sent:
            if fact.value and token.text in fact.value:
                fact_tokens.append(token)
        
        if not fact_tokens:
            return
        
        # Find related verb
        for token in fact_tokens:
            # Walk up the dependency tree
            current = token
            while current.head != current:
                if current.head.pos_ == 'VERB':
                    fact.predicate = current.head.text
                    
                    # Find subject
                    for child in current.head.children:
                        if child.dep_ in ["nsubj", "nsubjpass"]:
                            fact.subject = ' '.join([t.text for t in child.subtree])
                    
                    # Find object (if fact is not already the object)
                    for child in current.head.children:
                        if child.dep_ in ["dobj", "attr"] and child != token:
                            fact.object = ' '.join([t.text for t in child.subtree])
                    break
                current = current.head
    
    def _normalize_money(self, money_str: str) -> float:
        """Normalize money values to float"""
        try:
            # Remove currency symbols and commas
            clean = re.sub(r'[$,]', '', money_str)
            
            # Extract number
            number_match = re.search(r'[\d.]+', clean)
            if not number_match:
                return 0.0
            
            value = float(number_match.group())
            
            # Apply multipliers
            if re.search(r'(?:million|M)\b', money_str, re.IGNORECASE):
                value *= 1_000_000
            elif re.search(r'(?:billion|B)\b', money_str, re.IGNORECASE):
                value *= 1_000_000_000
            elif re.search(r'(?:thousand|K)\b', money_str, re.IGNORECASE):
                value *= 1_000
            
            return value
        except:
            return 0.0
    
    def _normalize_percentage(self, pct_str: str) -> float:
        """Normalize percentage to float"""
        try:
            number_match = re.search(r'[\d.]+', pct_str)
            if number_match:
                return float(number_match.group())
            return 0.0
        except:
            return 0.0
    
    def _normalize_date(self, date_str: str) -> str:
        """Normalize date to ISO format"""
        try:
            # Handle quarters
            quarter_match = re.match(r'Q(\d)\s*(\d{4})', date_str)
            if quarter_match:
                quarter = int(quarter_match.group(1))
                year = int(quarter_match.group(2))
                # Return as quarter representation
                return f"{year}-Q{quarter}"
            
            # Parse other date formats
            parsed = date_parser.parse(date_str, fuzzy=True)
            return parsed.strftime('%Y-%m-%d')
        except:
            return date_str.lower()
    
    def _normalize_quantity(self, qty_str: str) -> float:
        """Normalize quantities to float"""
        try:
            number_match = re.search(r'[\d,]+', qty_str)
            if number_match:
                return float(number_match.group().replace(',', ''))
            return 0.0
        except:
            return 0.0
    
    def verify_fact(self, fact: Fact, reference_facts: List[Fact]) -> FactVerification:
        """Verify a single fact against reference facts"""
        verification = FactVerification(
            fact=fact,
            status=VerificationStatus.UNSUPPORTED,
            confidence=0.0
        )
        
        # Find relevant reference facts
        relevant_facts = []
        for ref_fact in reference_facts:
            relevance_score = self._calculate_fact_relevance(fact, ref_fact)
            if relevance_score > 0.3:  # Some relevance
                relevant_facts.append((ref_fact, relevance_score))
        
        # Sort by relevance
        relevant_facts.sort(key=lambda x: x[1], reverse=True)
        
        if not relevant_facts:
            # No relevant facts found - could be new information
            verification.status = VerificationStatus.UNSUPPORTED
            verification.explanation = "No related information found in reference"
            return verification
        
        # Check against most relevant facts
        for ref_fact, relevance in relevant_facts[:3]:  # Top 3 most relevant
            comparison = self._compare_facts(fact, ref_fact)
            
            if comparison['identical']:
                verification.status = VerificationStatus.VERIFIED
                verification.supporting_facts.append(ref_fact)
                verification.confidence = 1.0
                verification.explanation = "Fact directly verified"
                return verification
            
            elif comparison['contradicts']:
                verification.status = VerificationStatus.CONTRADICTED
                verification.contradicting_facts.append(ref_fact)
                verification.confidence = comparison['confidence']
                verification.explanation = f"Contradicts: {ref_fact.text}"
                return verification
            
            elif comparison['partial_match']:
                if verification.status == VerificationStatus.UNSUPPORTED:
                    verification.status = VerificationStatus.PARTIAL
                    verification.supporting_facts.append(ref_fact)
                    verification.confidence = comparison['confidence']
        
        # Check if it's a reasonable inference
        if verification.status == VerificationStatus.UNSUPPORTED:
            inference_score = self._check_inference(fact, reference_facts)
            if inference_score > self.thresholds['inference']:
                verification.status = VerificationStatus.INFERRED
                verification.confidence = inference_score
                verification.explanation = "Reasonable inference from reference"
        
        return verification
    
    def _calculate_fact_relevance(self, fact1: Fact, fact2: Fact) -> float:
        """Calculate how relevant two facts are to each other"""
        relevance_scores = []
        
        # Type match
        if fact1.fact_type == fact2.fact_type:
            relevance_scores.append(0.3)
        
        # Subject similarity
        if fact1.subject and fact2.subject:
            subject_sim = self._semantic_similarity(fact1.subject, fact2.subject)
            relevance_scores.append(subject_sim * 0.4)
        
        # Context similarity
        if fact1.context and fact2.context:
            context_sim = self._semantic_similarity(fact1.context, fact2.context)
            relevance_scores.append(context_sim * 0.3)
        
        # Predicate match
        if fact1.predicate and fact2.predicate:
            if fact1.predicate.lower() == fact2.predicate.lower():
                relevance_scores.append(0.3)
            else:
                pred_sim = self._semantic_similarity(fact1.predicate, fact2.predicate)
                relevance_scores.append(pred_sim * 0.2)
        
        return sum(relevance_scores)
    
    def _compare_facts(self, fact1: Fact, fact2: Fact) -> Dict[str, Union[bool, float]]:
        """Compare two facts for verification"""
        result = {
            'identical': False,
            'contradicts': False,
            'partial_match': False,
            'confidence': 0.0
        }
        
        # For quantitative facts
        if fact1.fact_type == 'quantitative' and fact2.fact_type == 'quantitative':
            if fact1.normalized_value is not None and fact2.normalized_value is not None:
                # Check if discussing same metric
                context_sim = self._semantic_similarity(fact1.context, fact2.context)
                
                if context_sim > 0.7:  # Same context
                    if fact1.normalized_value == fact2.normalized_value:
                        result['identical'] = True
                        result['confidence'] = 1.0
                    else:
                        # Different values for same metric = contradiction
                        result['contradicts'] = True
                        result['confidence'] = context_sim
                else:
                    # Different contexts - might be different metrics
                    result['partial_match'] = context_sim > 0.5
                    result['confidence'] = context_sim
        
        # For relational facts
        elif fact1.fact_type == 'relational' and fact2.fact_type == 'relational':
            subject_match = False
            predicate_match = False
            object_match = False
            
            if fact1.subject and fact2.subject:
                subject_sim = self._semantic_similarity(fact1.subject, fact2.subject)
                subject_match = subject_sim > 0.8
            
            if fact1.predicate and fact2.predicate:
                predicate_sim = self._semantic_similarity(fact1.predicate, fact2.predicate)
                predicate_match = predicate_sim > 0.7
            
            if fact1.object and fact2.object:
                object_sim = self._semantic_similarity(fact1.object, fact2.object)
                object_match = object_sim > 0.8
            
            if subject_match and predicate_match:
                if object_match:
                    result['identical'] = True
                    result['confidence'] = 1.0
                else:
                    # Same subject and predicate but different object = potential contradiction
                    if fact1.object and fact2.object:
                        result['contradicts'] = True
                        result['confidence'] = 0.8
        
        return result
    
    def _semantic_similarity(self, text1: str, text2: str) -> float:
        """Calculate semantic similarity between texts"""
        if not text1 or not text2:
            return 0.0
        
        # Handle very short texts
        if len(text1.split()) < 3 or len(text2.split()) < 3:
            # Use fuzzy string matching for short texts
            return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
        
        # Use sentence transformer for longer texts
        embeddings = self.sentence_model.encode([text1, text2])
        similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        return float(similarity)
    
    def _check_inference(self, fact: Fact, reference_facts: List[Fact]) -> float:
        """Check if a fact is a reasonable inference from reference"""
        # Use NLI to check if reference facts entail this fact
        reference_text = ' '.join([f.text for f in reference_facts[:5]])  # Top 5 facts
        
        if len(reference_text) < 20:
            return 0.0
        
        nli_result = self.nli_pipeline(
            f"{reference_text} [SEP] {fact.text}",
            max_length=512,
            truncation=True
        )
        
        for result in nli_result:
            if result['label'] == 'ENTAILMENT':
                return result['score']
            elif result['label'] == 'NEUTRAL':
                return result['score'] * 0.5  # Partial credit for neutral
        
        return 0.0
    
    def analyze_hallucination_patterns(self, 
                                     verifications: List[FactVerification],
                                     text_metrics: Dict[str, float]) -> Dict[str, float]:
        """Analyze patterns in hallucinations"""
        patterns = {
            'factual_accuracy': 0.0,
            'consistency': 0.0,
            'completeness': 0.0,
            'inference_quality': 0.0,
            'specificity': 0.0
        }
        
        if not verifications:
            return patterns
        
        # Factual accuracy
        verified_count = sum(1 for v in verifications if v.status == VerificationStatus.VERIFIED)
        contradicted_count = sum(1 for v in verifications if v.status == VerificationStatus.CONTRADICTED)
        total_facts = len(verifications)
        
        patterns['factual_accuracy'] = verified_count / total_facts if total_facts > 0 else 0
        
        # Consistency (no contradictions)
        patterns['consistency'] = 1.0 - (contradicted_count / total_facts) if total_facts > 0 else 1.0
        
        # Completeness (not too many unsupported facts)
        unsupported_count = sum(1 for v in verifications if v.status == VerificationStatus.UNSUPPORTED)
        patterns['completeness'] = 1.0 - (unsupported_count / total_facts) if total_facts > 0 else 1.0
        
        # Inference quality
        inferred_count = sum(1 for v in verifications if v.status == VerificationStatus.INFERRED)
        if inferred_count > 0:
            avg_inference_confidence = np.mean([v.confidence for v in verifications 
                                              if v.status == VerificationStatus.INFERRED])
            patterns['inference_quality'] = avg_inference_confidence
        
        # Specificity (using quantitative facts)
        quantitative_facts = sum(1 for v in verifications if v.fact.fact_type == 'quantitative')
        patterns['specificity'] = quantitative_facts / total_facts if total_facts > 0 else 0
        
        return patterns
    
    def calculate_hallucination_severity(self, 
                                       verifications: List[FactVerification],
                                       patterns: Dict[str, float]) -> Tuple[str, float]:
        """Calculate the severity of hallucination"""
        # Count verification statuses
        status_counts = defaultdict(int)
        for v in verifications:
            status_counts[v.status] += 1
        
        total_facts = len(verifications) if verifications else 1
        
        # Severity scoring
        severity_score = 0.0
        
        # Heavy penalty for contradictions
        contradiction_ratio = status_counts[VerificationStatus.CONTRADICTED] / total_facts
        severity_score += contradiction_ratio * 0.5
        
        # Moderate penalty for unsupported facts
        unsupported_ratio = status_counts[VerificationStatus.UNSUPPORTED] / total_facts
        severity_score += unsupported_ratio * 0.3
        
        # Light penalty for poor patterns
        if patterns['factual_accuracy'] < 0.5:
            severity_score += 0.2
        
        if patterns['consistency'] < 0.7:
            severity_score += 0.1
        
        # Determine severity level
        if severity_score < 0.1:
            return 'none', severity_score
        elif severity_score < 0.3:
            return 'minor', severity_score
        elif severity_score < 0.6:
            return 'moderate', severity_score
        else:
            return 'severe', severity_score
    
    def build_verification_graph(self, 
                               paragraphs: List[str],
                               analyses: List[HallucinationAnalysis]) -> nx.Graph:
        """Build graph with verification-based edges"""
        G = nx.Graph()
        
        # Add nodes
        for i, (text, analysis) in enumerate(zip(paragraphs, analyses)):
            if i == 0:  # Reference
                G.add_node(i,
                          text=text,
                          is_reference=True,
                          is_hallucinated=False,
                          severity='none',
                          patterns=analysis.hallucination_patterns if analysis else {})
            else:
                G.add_node(i,
                          text=text,
                          is_reference=False,
                          is_hallucinated=analysis.is_hallucinated,
                          severity=analysis.severity,
                          confidence=analysis.confidence,
                          patterns=analysis.hallucination_patterns)
        
        # Add edges based on fact overlap
        all_facts = []
        for i, text in enumerate(paragraphs):
            facts = self.extract_facts(text)
            all_facts.append(facts)
        
        # Create edges based on shared facts
        for i in range(len(paragraphs)):
            for j in range(i + 1, len(paragraphs)):
                shared_facts = 0
                conflicting_facts = 0
                
                # Compare facts between paragraphs
                for fact_i in all_facts[i]:
                    for fact_j in all_facts[j]:
                        relevance = self._calculate_fact_relevance(fact_i, fact_j)
                        if relevance > 0.7:
                            comparison = self._compare_facts(fact_i, fact_j)
                            if comparison['identical']:
                                shared_facts += 1
                            elif comparison['contradicts']:
                                conflicting_facts += 1
                
                # Add edge if there's significant relationship
                if shared_facts > 0 or conflicting_facts > 0:
                    total_facts = max(len(all_facts[i]), len(all_facts[j]))
                    weight = shared_facts / total_facts if total_facts > 0 else 0
                    
                    G.add_edge(i, j,
                             weight=weight,
                             shared_facts=shared_facts,
                             conflicting_facts=conflicting_facts,
                             edge_type='conflict' if conflicting_facts > shared_facts else 'support')
        
        return G
    
    def analyze_paragraph(self, reference: str, candidate: str) -> HallucinationAnalysis:
        """Analyze a single paragraph for hallucination"""
        # Extract facts
        ref_facts = self.extract_facts(reference)
        cand_facts = self.extract_facts(candidate)
        
        # Verify each candidate fact
        verifications = []
        for fact in cand_facts:
            verification = self.verify_fact(fact, ref_facts)
            verifications.append(verification)
        
        # Calculate text metrics
        text_metrics = {
            'semantic_similarity': self._semantic_similarity(reference, candidate),
            'length_ratio': len(candidate.split()) / len(reference.split()),
            'fact_density': len(cand_facts) / max(len(candidate.split()), 1)
        }
        
        # Analyze patterns
        patterns = self.analyze_hallucination_patterns(verifications, text_metrics)
        
        # Calculate severity
        severity, severity_score = self.calculate_hallucination_severity(verifications, patterns)
        
        # Determine if hallucinated
        is_hallucinated = (
            severity_score > self.thresholds['hallucination'] or
            patterns['consistency'] < 0.5 or
            patterns['factual_accuracy'] < 0.3
        )
        
        # Generate explanation
        explanation = self._generate_explanation(verifications, patterns, severity)
        
        return HallucinationAnalysis(
            is_hallucinated=is_hallucinated,
            confidence=min(severity_score + 0.3, 1.0) if is_hallucinated else 1.0 - severity_score,
            verification_results=verifications,
            hallucination_patterns=patterns,
            graph_metrics={},  # Will be filled by graph analysis
            explanation=explanation,
            severity=severity
        )
    
    def _generate_explanation(self, 
                            verifications: List[FactVerification],
                            patterns: Dict[str, float],
                            severity: str) -> str:
        """Generate detailed explanation of the analysis"""
        explanation_parts = []
        
        # Summary
        status_counts = defaultdict(int)
        for v in verifications:
            status_counts[v.status] += 1
        
        total_facts = len(verifications)
        
        if total_facts > 0:
            explanation_parts.append(f"Analyzed {total_facts} facts:")
            explanation_parts.append(f"- Verified: {status_counts[VerificationStatus.VERIFIED]}")
            explanation_parts.append(f"- Contradicted: {status_counts[VerificationStatus.CONTRADICTED]}")
            explanation_parts.append(f"- Unsupported: {status_counts[VerificationStatus.UNSUPPORTED]}")
            explanation_parts.append(f"- Inferred: {status_counts[VerificationStatus.INFERRED]}")
        
        # Key issues
        if status_counts[VerificationStatus.CONTRADICTED] > 0:
            explanation_parts.append("\nKey contradictions found:")
            for v in verifications[:3]:  # Show up to 3
                if v.status == VerificationStatus.CONTRADICTED:
                    explanation_parts.append(f"- '{v.fact.text}' contradicts reference")
        
        # Pattern analysis
        explanation_parts.append(f"\nPattern Analysis:")
        explanation_parts.append(f"- Factual accuracy: {patterns['factual_accuracy']:.2%}")
        explanation_parts.append(f"- Consistency: {patterns['consistency']:.2%}")
        explanation_parts.append(f"- Specificity: {patterns['specificity']:.2%}")
        
        # Severity
        explanation_parts.append(f"\nSeverity: {severity.upper()}")
        
        return '\n'.join(explanation_parts)
    
    def create_interactive_visualization(self, 
                                       G: nx.Graph,
                                       analyses: List[HallucinationAnalysis],
                                       save_path: str = "foolproof_hallucination.html") -> str:
        """Create interactive visualization with verification details"""
        # Calculate graph metrics
        if G.number_of_edges() > 0:
            pagerank = nx.pagerank(G, weight='weight')
            betweenness = nx.betweenness_centrality(G)
            closeness = nx.closeness_centrality(G)
        else:
            pagerank = {n: 1/G.number_of_nodes() for n in G.nodes()}
            betweenness = {n: 0 for n in G.nodes()}
            closeness = {n: 0 for n in G.nodes()}
        
        # Prepare nodes data
        nodes = []
        for node in G.nodes():
            node_data = G.nodes[node]
            
            # Color based on severity
            if node_data.get('is_reference'):
                color = '#0066CC'
            elif node_data.get('severity') == 'severe':
                color = '#CC0000'
            elif node_data.get('severity') == 'moderate':
                color = '#FF6666'
            elif node_data.get('severity') == 'minor':
                color = '#FFAAAA'
            else:
                color = '#00CC00'
            
            nodes.append({
                'id': node,
                'label': f'P{node}',
                'color': color,
                'size': 10 + 30 * pagerank[node],
                'severity': node_data.get('severity', 'none'),
                'confidence': node_data.get('confidence', 1.0),
                'is_hallucinated': node_data.get('is_hallucinated', False),
                'patterns': node_data.get('patterns', {}),
                'text': node_data['text'][:200] + '...' if len(node_data['text']) > 200 else node_data['text'],
                'pagerank': pagerank[node],
                'betweenness': betweenness[node],
                'closeness': closeness[node]
            })
        
        # Prepare edges data
        links = []
        for u, v, data in G.edges(data=True):
            color = '#FF0000' if data.get('edge_type') == 'conflict' else '#00FF00'
            links.append({
                'source': u,
                'target': v,
                'weight': data.get('weight', 0),
                'shared_facts': data.get('shared_facts', 0),
                'conflicting_facts': data.get('conflicting_facts', 0),
                'color': color,
                'width': 1 + data.get('weight', 0) * 5
            })
        
        # Create HTML
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Fool-Proof Hallucination Detection</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <style>
        body {{
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        #graph {{
            background-color: white;
            border: 1px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .tooltip {{
            position: absolute;
            text-align: left;
            padding: 12px;
            font: 12px sans-serif;
            background: rgba(0, 0, 0, 0.95);
            color: white;
            border-radius: 8px;
            pointer-events: none;
            opacity: 0;
            transition: opacity 0.3s;
            max-width: 500px;
            line-height: 1.5;
        }}
        .metric-group {{
            margin: 10px 0;
            padding: 8px;
            border: 1px solid #444;
            border-radius: 4px;
        }}
        .metric-row {{
            display: flex;
            justify-content: space-between;
            margin: 3px 0;
        }}
        .metric-label {{
            font-weight: bold;
            color: #66ccff;
        }}
        .metric-value {{
            color: #ffffff;
        }}
        .severity-badge {{
            display: inline-block;
            padding: 3px 8px;
            border-radius: 4px;
            font-weight: bold;
            margin: 5px 0;
        }}
        .severity-none {{ background: #00CC00; color: white; }}
        .severity-minor {{ background: #FFAA00; color: black; }}
        .severity-moderate {{ background: #FF6600; color: white; }}
        .severity-severe {{ background: #CC0000; color: white; }}
        .legend {{
            position: absolute;
            top: 20px;
            right: 20px;
            background: rgba(255, 255, 255, 0.95);
            padding: 15px;
            border-radius: 8px;
            border: 1px solid #ddd;
            max-width: 250px;
        }}
        .legend-item {{
            margin: 5px 0;
            display: flex;
            align-items: center;
        }}
        .legend-color {{
            width: 20px;
            height: 20px;
            margin-right: 8px;
            border: 1px solid #333;
            border-radius: 3px;
        }}
        h1 {{
            color: #333;
            margin-bottom: 10px;
        }}
        .controls {{
            margin-bottom: 20px;
        }}
        button {{
            padding: 8px 16px;
            margin-right: 10px;
            border: none;
            border-radius: 4px;
            background-color: #4CAF50;
            color: white;
            cursor: pointer;
            font-size: 14px;
        }}
        button:hover {{
            background-color: #45a049;
        }}
    </style>
</head>
<body>
    <h1>Fool-Proof Hallucination Detection - Fact Verification System</h1>
    <div class="controls">
        <button onclick="resetSimulation()">Reset View</button>
        <button onclick="toggleLabels()">Toggle Labels</button>
    </div>
    <div id="graph"></div>
    <div class="tooltip"></div>
    <div class="legend">
        <h3 style="margin-top: 0;">Severity Levels</h3>
        <div class="legend-item">
            <div class="legend-color" style="background-color: #0066CC;"></div>
            <span>Reference</span>
        </div>
        <div class="legend-item">
            <div class="legend-color" style="background-color: #CC0000;"></div>
            <span>Severe Hallucination</span>
        </div>
        <div class="legend-item">
            <div class="legend-color" style="background-color: #FF6666;"></div>
            <span>Moderate Hallucination</span>
        </div>
        <div class="legend-item">
            <div class="legend-color" style="background-color: #FFAAAA;"></div>
            <span>Minor Issues</span>
        </div>
        <div class="legend-item">
            <div class="legend-color" style="background-color: #00CC00;"></div>
            <span>Verified/Consistent</span>
        </div>
        <h3>Edge Types</h3>
        <div class="legend-item">
            <div class="legend-color" style="background-color: #00FF00;"></div>
            <span>Supporting Facts</span>
        </div>
        <div class="legend-item">
            <div class="legend-color" style="background-color: #FF0000;"></div>
            <span>Conflicting Facts</span>
        </div>
    </div>
    
    <script>
        const nodes = {json.dumps(nodes)};
        const links = {json.dumps(links)};
        
        const width = window.innerWidth - 60;
        const height = window.innerHeight - 200;
        
        const svg = d3.select("#graph")
            .append("svg")
            .attr("width", width)
            .attr("height", height);
        
        const zoom = d3.zoom()
            .scaleExtent([0.1, 10])
            .on("zoom", zoomed);
        
        svg.call(zoom);
        
        const g = svg.append("g");
        
        const tooltip = d3.select(".tooltip");
        
        const simulation = d3.forceSimulation(nodes)
            .force("link", d3.forceLink(links)
                .id(d => d.id)
                .distance(d => 120 / (d.weight + 0.1))
                .strength(d => d.weight))
            .force("charge", d3.forceManyBody()
                .strength(d => -400 - d.size * 5))
            .force("center", d3.forceCenter(width / 2, height / 2))
            .force("collision", d3.forceCollide()
                .radius(d => d.size + 10));
        
        const link = g.append("g")
            .selectAll("line")
            .data(links)
            .enter().append("line")
            .attr("stroke", d => d.color)
            .attr("stroke-width", d => d.width)
            .attr("stroke-opacity", 0.6);
        
        const node = g.append("g")
            .selectAll("circle")
            .data(nodes)
            .enter().append("circle")
            .attr("r", d => d.size)
            .attr("fill", d => d.color)
            .attr("stroke", "#333")
            .attr("stroke-width", 2)
            .style("cursor", "pointer")
            .call(drag(simulation));
        
        const labels = g.append("g")
            .selectAll("text")
            .data(nodes)
            .enter().append("text")
            .text(d => d.label)
            .attr("font-size", 12)
            .attr("dx", d => d.size + 5)
            .attr("dy", 4)
            .style("pointer-events", "none");
        
        node.on("mouseover", function(event, d) {{
            let html = `<strong>${{d.label}}</strong>`;
            html += `<div class="severity-badge severity-${{d.severity}}">${{d.severity.toUpperCase()}}</div><br/>`;
            
            html += `<div class="metric-group">`;
            html += `<strong>Verification Results:</strong><br/>`;
            html += `<div class="metric-row"><span class="metric-label">Hallucinated:</span><span class="metric-value">${{d.is_hallucinated ? 'Yes' : 'No'}}</span></div>`;
            html += `<div class="metric-row"><span class="metric-label">Confidence:</span><span class="metric-value">${{(d.confidence * 100).toFixed(1)}}%</span></div>`;
            html += `</div>`;
            
            if (d.patterns) {{
                html += `<div class="metric-group">`;
                html += `<strong>Pattern Analysis:</strong><br/>`;
                html += `<div class="metric-row"><span class="metric-label">Factual Accuracy:</span><span class="metric-value">${{(d.patterns.factual_accuracy * 100).toFixed(1)}}%</span></div>`;
                html += `<div class="metric-row"><span class="metric-label">Consistency:</span><span class="metric-value">${{(d.patterns.consistency * 100).toFixed(1)}}%</span></div>`;
                html += `<div class="metric-row"><span class="metric-label">Specificity:</span><span class="metric-value">${{(d.patterns.specificity * 100).toFixed(1)}}%</span></div>`;
                html += `</div>`;
            }}
            
            html += `<div class="metric-group">`;
            html += `<strong>Graph Metrics:</strong><br/>`;
            html += `<div class="metric-row"><span class="metric-label">PageRank:</span><span class="metric-value">${{d.pagerank.toFixed(3)}}</span></div>`;
            html += `<div class="metric-row"><span class="metric-label">Betweenness:</span><span class="metric-value">${{d.betweenness.toFixed(3)}}</span></div>`;
            html += `<div class="metric-row"><span class="metric-label">Closeness:</span><span class="metric-value">${{d.closeness.toFixed(3)}}</span></div>`;
            html += `</div>`;
            
            html += `<div class="metric-group">`;
            html += `<strong>Text:</strong><br/>${{d.text}}`;
            html += `</div>`;
            
            tooltip.html(html)
                .style("left", (event.pageX + 10) + "px")
                .style("top", (event.pageY - 10) + "px")
                .style("opacity", 1);
            
            d3.select(this).attr("stroke-width", 4);
            
            link.style("stroke-opacity", l => 
                (l.source.id === d.id || l.target.id === d.id) ? 1 : 0.2
            );
            
            node.style("opacity", n => {{
                if (n.id === d.id) return 1;
                const connected = links.some(l => 
                    (l.source.id === d.id && l.target.id === n.id) ||
                    (l.target.id === d.id && l.source.id === n.id)
                );
                return connected ? 1 : 0.3;
            }});
        }})
        .on("mouseout", function() {{
            tooltip.style("opacity", 0);
            d3.select(this).attr("stroke-width", 2);
            link.style("stroke-opacity", 0.6);
            node.style("opacity", 1);
        }});
        
        link.on("mouseover", function(event, d) {{
            let html = `<strong>Edge: P${{d.source.id}} ↔ P${{d.target.id}}</strong><br/>`;
            html += `<div class="metric-row"><span class="metric-label">Shared Facts:</span><span class="metric-value">${{d.shared_facts}}</span></div>`;
            html += `<div class="metric-row"><span class="metric-label">Conflicting Facts:</span><span class="metric-value">${{d.conflicting_facts}}</span></div>`;
            html += `<div class="metric-row"><span class="metric-label">Weight:</span><span class="metric-value">${{d.weight.toFixed(3)}}</span></div>`;
            
            tooltip.html(html)
                .style("left", (event.pageX + 10) + "px")
                .style("top", (event.pageY - 10) + "px")
                .style("opacity", 1);
                
            d3.select(this).attr("stroke-width", d => d.width * 2);
        }})
        .on("mouseout", function(event, d) {{
            tooltip.style("opacity", 0);
            d3.select(this).attr("stroke-width", d => d.width);
        }});
        
        simulation.on("tick", () => {{
            link
                .attr("x1", d => d.source.x)
                .attr("y1", d => d.source.y)
                .attr("x2", d => d.target.x)
                .attr("y2", d => d.target.y);
            
            node
                .attr("cx", d => d.x)
                .attr("cy", d => d.y);
            
            labels
                .attr("x", d => d.x)
                .attr("y", d => d.y);
        }});
        
        function drag(simulation) {{
            function dragstarted(event, d) {{
                if (!event.active) simulation.alphaTarget(0.3).restart();
                d.fx = d.x;
                d.fy = d.y;
            }}
            
            function dragged(event, d) {{
                d.fx = event.x;
                d.fy = event.y;
            }}
            
            function dragended(event, d) {{
                if (!event.active) simulation.alphaTarget(0);
                d.fx = null;
                d.fy = null;
            }}
            
            return d3.drag()
                .on("start", dragstarted)
                .on("drag", dragged)
                .on("end", dragended);
        }}
        
        function zoomed(event) {{
            g.attr("transform", event.transform);
        }}
        
        function resetSimulation() {{
            simulation.alpha(1).restart();
            svg.transition().duration(750).call(
                zoom.transform,
                d3.zoomIdentity
            );
        }}
        
        let labelsVisible = true;
        function toggleLabels() {{
            labelsVisible = !labelsVisible;
            labels.style("display", labelsVisible ? "block" : "none");
        }}
    </script>
</body>
</html>
"""
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return save_path
    
    def analyze(self, reference: str, candidates: List[str], output_dir: str = ".") -> Dict:
        """Complete fool-proof analysis pipeline"""
        print("\n" + "="*60)
        print("FOOL-PROOF HALLUCINATION DETECTION ANALYSIS")
        print("="*60)
        
        all_paragraphs = [reference] + candidates
        analyses = [None]  # Reference has no analysis
        
        # Analyze each candidate
        print("\nAnalyzing candidates...")
        for i, candidate in enumerate(candidates):
            print(f"  Analyzing paragraph {i+1}...")
            analysis = self.analyze_paragraph(reference, candidate)
            analyses.append(analysis)
        
        # Build verification graph
        print("Building verification graph...")
        G = self.build_verification_graph(all_paragraphs, analyses)
        
        # Create visualization
        print("Creating interactive visualization...")
        viz_path = os.path.join(output_dir, "foolproof_hallucination.html")
        self.create_interactive_visualization(G, analyses[1:], viz_path)
        
        # Print summary
        print("\n" + "-"*60)
        print("ANALYSIS SUMMARY")
        print("-"*60)
        
        severity_counts = defaultdict(int)
        for analysis in analyses[1:]:
            severity_counts[analysis.severity] += 1
        
        print(f"Total candidates: {len(candidates)}")
        print(f"Severity breakdown:")
        for severity in ['none', 'minor', 'moderate', 'severe']:
            print(f"  {severity.capitalize()}: {severity_counts[severity]}")
        
        # Show detailed results for each paragraph
        print("\nDetailed Results:")
        for i, analysis in enumerate(analyses[1:], 1):
            print(f"\nParagraph {i}:")
            print(f"  Hallucinated: {'Yes' if analysis.is_hallucinated else 'No'}")
            print(f"  Severity: {analysis.severity}")
            print(f"  Confidence: {analysis.confidence:.2%}")
            print(f"  Pattern scores:")
            for pattern, score in analysis.hallucination_patterns.items():
                print(f"    {pattern}: {score:.2%}")
        
        print(f"\nVisualization saved to: {viz_path}")
        print("Opening in browser...")
        webbrowser.open(f'file://{os.path.abspath(viz_path)}')
        
        return {
            'analyses': analyses[1:],
            'graph': G,
            'visualization': viz_path,
            'summary': dict(severity_counts)
        }


# Example usage
if __name__ == "__main__":
    detector = FoolProofHallucinationDetector(use_gpu=False)
    
    # Test with your examples
    reference = """The company reported revenue of $2.5 million in Q4 2023, with a 15% increase from the previous quarter. 
    CEO John Smith announced expansion plans on January 15, 2024, targeting the Asian market. 
    The profit margin improved to 22% due to cost optimization strategies."""
    
    candidates = [
        # Factual contradiction
        """The company reported revenue of $3.2 million in Q4 2023, with a 15% increase from the previous quarter. 
        CEO John Smith announced expansion plans on January 20, 2024, targeting the Asian market. 
        The profit margin improved to 25% due to cost optimization strategies.""",
        
        # Subtle hallucination
        """Q4 2023 saw revenue of $2.5 million, representing 15% growth. 
        John Smith revealed plans for Asian expansion in January 2024, building on the company's momentum. 
        Margins reached 22% through operational efficiencies and the company expects continued improvement.""",
        
        # Additional non-contradictory information
        """The company's Q4 2023 revenue of $2.5 million marked a 15% quarterly increase. 
        CEO John Smith's January 15, 2024 announcement about Asian market expansion was well-received by investors. 
        The 22% profit margin improvement came from cost optimization in manufacturing and logistics.""",
        
        # Complete fabrication
        """In Q4 2023, the company achieved record-breaking performance with multiple revenue streams. 
        The board of directors approved a comprehensive restructuring plan. 
        New partnerships were established with leading technology firms to drive innovation."""
    ]
    
    results = detector.analyze(reference, candidates)
