"""
Strict Hallucination Detection System with Interactive Visualization
===================================================================
A complete system with stricter hallucination detection:
- Strict fact checking for dates, numbers, and entities
- Final consistency score for the entire set
- Clear classification logic
- Interactive force-directed graph
"""

import numpy as np
from typing import List, Dict, Tuple, Set, Any
import re
import networkx as nx
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from collections import defaultdict
import warnings
from scipy.stats import entropy
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import torch
import webbrowser
import os
import json
import textwrap
from datetime import datetime

warnings.filterwarnings('ignore')

# Download required NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('maxent_ne_chunker', quiet=True)
nltk.download('words', quiet=True)


class StrictHallucinationDetector:
    """Strict Hallucination Detection System with Final Scoring"""
    
    def __init__(self, use_gpu=False):
        """Initialize the detector with all models"""
        self.device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'
        print(f"Initializing Strict Hallucination Detector on {self.device}...")
        
        # Models
        print("Loading models...")
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.sentence_model.to(self.device)
        
        self.nli_pipeline = pipeline(
            "text-classification", 
            model="cross-encoder/nli-deberta-v3-base",
            device=0 if self.device == 'cuda' else -1
        )
        
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 3),
            stop_words='english'
        )
        
        # Patterns for entity extraction
        self.patterns = {
            'date': r'\b(?:\d{1,2}[-/]\d{1,2}[-/]\d{2,4}|\d{4}[-/]\d{1,2}[-/]\d{1,2}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4})\b',
            'money': r'\$[\d,]+(?:\.\d{2})?|\b\d+(?:,\d{3})*(?:\.\d{2})?\s*(?:dollars?|USD|cents?|million|billion)\b',
            'percentage': r'\b\d+(?:\.\d+)?%',
            'number': r'\b\d+(?:,\d{3})*(?:\.\d+)?\b'
        }
        
        # Strict thresholds
        self.thresholds = {
            'semantic_high': 0.8,      # Very similar content
            'semantic_medium': 0.6,    # Related content
            'semantic_low': 0.4,       # Somewhat related
            'factual_strict': 0.9,     # For critical facts
            'factual_moderate': 0.7,   # For general facts
            'contradiction': 0.3,      # Max allowed contradiction
            'hallucination': 0.4       # Lower threshold = stricter
        }
        
        print("Initialization complete!")
    
    def extract_entities(self, text: str) -> Dict[str, Any]:
        """Extract entities with normalized values"""
        entities = {
            'dates': set(),
            'money': set(),
            'money_values': set(),  # Normalized money values
            'percentages': set(),
            'numbers': set(),
            'named_entities': set(),
            'facts': []
        }
        
        # Extract dates
        entities['dates'] = set(re.findall(self.patterns['date'], text, re.IGNORECASE))
        
        # Extract money with normalization
        money_matches = re.findall(self.patterns['money'], text, re.IGNORECASE)
        for match in money_matches:
            entities['money'].add(match)
            # Extract numeric value
            value_match = re.search(r'([\d,]+(?:\.\d+)?)', match)
            if value_match:
                value = float(value_match.group(1).replace(',', ''))
                # Handle millions/billions
                if 'million' in match.lower():
                    value *= 1000000
                elif 'billion' in match.lower():
                    value *= 1000000000
                entities['money_values'].add(value)
        
        # Extract percentages
        entities['percentages'] = set(re.findall(self.patterns['percentage'], text))
        
        # Extract other numbers
        number_matches = re.findall(self.patterns['number'], text)
        for num in number_matches:
            # Skip if it's part of money or percentage
            if not any(num in m for m in entities['money']) and not any(num in p for p in entities['percentages']):
                try:
                    entities['numbers'].add(float(num.replace(',', '')))
                except:
                    pass
        
        # Extract named entities
        try:
            tokens = word_tokenize(text)
            pos_tags = nltk.pos_tag(tokens)
            chunks = nltk.ne_chunk(pos_tags, binary=False)
            
            for chunk in chunks:
                if hasattr(chunk, 'label'):
                    entity_name = ' '.join(c[0] for c in chunk)
                    entities['named_entities'].add((entity_name.lower(), chunk.label()))
        except:
            pass
        
        # Extract factual sentences
        sentences = sent_tokenize(text)
        for sent in sentences:
            if any(re.search(self.patterns[p], sent) for p in ['date', 'money', 'percentage']):
                entities['facts'].append(sent.strip())
        
        return entities
    
    def calculate_entropy(self, text: str) -> float:
        """Calculate text entropy"""
        words = word_tokenize(text.lower())
        stopwords = set(nltk.corpus.stopwords.words('english'))
        words = [w for w in words if w.isalnum() and w not in stopwords]
        
        if not words:
            return 0.0
        
        word_freq = defaultdict(int)
        for word in words:
            word_freq[word] += 1
        
        total = len(words)
        probs = [count/total for count in word_freq.values()]
        return entropy(probs)
    
    def semantic_similarity(self, text1: str, text2: str) -> float:
        """Calculate semantic similarity"""
        if not text1.strip() or not text2.strip():
            return 0.0
        
        try:
            embeddings = self.sentence_model.encode([text1, text2])
            transformer_sim = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
        except:
            return 0.0
        
        return float(transformer_sim)
    
    def check_entailment(self, premise: str, hypothesis: str) -> Dict[str, float]:
        """Check entailment between texts"""
        try:
            results = self.nli_pipeline(f"{premise} [SEP] {hypothesis}")
            
            scores = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}
            mapping = {'ENTAILMENT': 'entailment', 'NEUTRAL': 'neutral', 'CONTRADICTION': 'contradiction'}
            
            for result in results:
                label = result['label'].upper()
                if label in mapping:
                    scores[mapping[label]] = result['score']
            
            return scores
        except:
            return {'entailment': 0.0, 'neutral': 1.0, 'contradiction': 0.0}
    
    def strict_fact_checking(self, ref_entities: Dict, cand_entities: Dict) -> Dict[str, Any]:
        """Strict fact checking with detailed results"""
        results = {
            'date_match': True,
            'money_match': True,
            'percentage_match': True,
            'number_match': True,
            'entity_match': True,
            'critical_errors': [],
            'overall_match': 1.0
        }
        
        # Check dates - must match exactly if present
        if ref_entities['dates'] and cand_entities['dates']:
            if ref_entities['dates'] != cand_entities['dates']:
                results['date_match'] = False
                results['critical_errors'].append(f"Date mismatch: {ref_entities['dates']} vs {cand_entities['dates']}")
        
        # Check money values - strict comparison
        if ref_entities['money_values'] and cand_entities['money_values']:
            ref_values = sorted(list(ref_entities['money_values']))
            cand_values = sorted(list(cand_entities['money_values']))
            
            # Allow small tolerance for rounding
            matched = True
            for rv in ref_values:
                found = False
                for cv in cand_values:
                    if abs(rv - cv) / max(rv, cv) < 0.01:  # 1% tolerance
                        found = True
                        break
                if not found:
                    matched = False
                    break
            
            if not matched:
                results['money_match'] = False
                results['critical_errors'].append(f"Money value mismatch: {ref_values} vs {cand_values}")
        
        # Check percentages - must match exactly
        if ref_entities['percentages'] and cand_entities['percentages']:
            if ref_entities['percentages'] != cand_entities['percentages']:
                results['percentage_match'] = False
                results['critical_errors'].append(f"Percentage mismatch: {ref_entities['percentages']} vs {cand_entities['percentages']}")
        
        # Check key named entities
        if ref_entities['named_entities'] and cand_entities['named_entities']:
            ref_names = {e[0] for e in ref_entities['named_entities'] if e[1] in ['PERSON', 'ORG']}
            cand_names = {e[0] for e in cand_entities['named_entities'] if e[1] in ['PERSON', 'ORG']}
            
            if ref_names and cand_names:
                overlap = len(ref_names & cand_names) / len(ref_names)
                if overlap < 0.5:  # Less than 50% overlap in key entities
                    results['entity_match'] = False
                    results['critical_errors'].append(f"Entity mismatch: missing {ref_names - cand_names}")
        
        # Calculate overall match score
        match_scores = []
        if ref_entities['dates'] or cand_entities['dates']:
            match_scores.append(1.0 if results['date_match'] else 0.0)
        if ref_entities['money_values'] or cand_entities['money_values']:
            match_scores.append(1.0 if results['money_match'] else 0.0)
        if ref_entities['percentages'] or cand_entities['percentages']:
            match_scores.append(1.0 if results['percentage_match'] else 0.0)
        if ref_entities['named_entities'] or cand_entities['named_entities']:
            match_scores.append(1.0 if results['entity_match'] else 0.0)
        
        if match_scores:
            results['overall_match'] = np.mean(match_scores)
        
        return results
    
    def classify_hallucination_strict(self, reference: str, candidate: str) -> Dict:
        """Strict hallucination classification"""
        ref_entities = self.extract_entities(reference)
        cand_entities = self.extract_entities(candidate)
        
        # Semantic similarity
        semantic_sim = self.semantic_similarity(reference, candidate)
        
        # Entailment checking
        entailment_scores = self.check_entailment(reference, candidate)
        
        # Strict fact checking
        fact_check = self.strict_fact_checking(ref_entities, cand_entities)
        
        # Decision logic
        is_hallucinated = False
        hallucination_score = 0.0
        reasons = []
        
        # High semantic similarity but factual errors = definite hallucination
        if semantic_sim > self.thresholds['semantic_medium']:
            if fact_check['critical_errors']:
                is_hallucinated = True
                hallucination_score = 0.8 + (0.2 * (1 - fact_check['overall_match']))
                reasons.extend(fact_check['critical_errors'])
            elif entailment_scores['contradiction'] > self.thresholds['contradiction']:
                is_hallucinated = True
                hallucination_score = 0.6 + (0.4 * entailment_scores['contradiction'])
                reasons.append(f"High contradiction: {entailment_scores['contradiction']:.2f}")
        
        # Medium similarity with significant factual errors
        elif semantic_sim > self.thresholds['semantic_low']:
            if not fact_check['overall_match'] > self.thresholds['factual_moderate']:
                is_hallucinated = True
                hallucination_score = 0.5 + (0.3 * (1 - fact_check['overall_match']))
                reasons.extend(fact_check['critical_errors'])
        
        # Low similarity - check if it's discussing the same topic incorrectly
        else:
            # If it mentions same entities but with different facts
            ref_names = {e[0] for e in ref_entities['named_entities']}
            cand_names = {e[0] for e in cand_entities['named_entities']}
            
            if ref_names & cand_names and fact_check['critical_errors']:
                is_hallucinated = True
                hallucination_score = 0.4 + (0.2 * (1 - semantic_sim))
                reasons.append("Same entities but different facts")
        
        # Final score calculation
        if not is_hallucinated:
            # Calculate score even for non-hallucinated
            hallucination_score = (
                (1 - semantic_sim) * 0.2 +
                (1 - fact_check['overall_match']) * 0.5 +
                entailment_scores['contradiction'] * 0.3
            )
        
        return {
            'is_hallucinated': is_hallucinated,
            'hallucination_score': hallucination_score,
            'semantic_similarity': semantic_sim,
            'entailment_scores': entailment_scores,
            'fact_check': fact_check,
            'reasons': reasons
        }
    
    def calculate_pairwise_consistency(self, text1: str, text2: str) -> Dict:
        """Calculate consistency between any two texts"""
        entities1 = self.extract_entities(text1)
        entities2 = self.extract_entities(text2)
        
        semantic_sim = self.semantic_similarity(text1, text2)
        
        # Only check facts if semantically similar
        if semantic_sim > self.thresholds['semantic_low']:
            fact_check = self.strict_fact_checking(entities1, entities2)
            factual_consistency = fact_check['overall_match']
        else:
            factual_consistency = 1.0  # Don't penalize unrelated content
        
        # Consistency score
        consistency_score = semantic_sim * 0.6 + factual_consistency * 0.4
        
        return {
            'consistency_score': consistency_score,
            'semantic_similarity': semantic_sim,
            'factual_consistency': factual_consistency
        }
    
    def build_graph_and_analyze(self, paragraphs: List[str]) -> Tuple[nx.Graph, List[Dict], Dict, float]:
        """Build graph and calculate final consistency score"""
        n = len(paragraphs)
        G = nx.Graph()
        
        # Add nodes
        for i, text in enumerate(paragraphs):
            G.add_node(i,
                      text=text,
                      is_reference=(i == 0),
                      entropy=self.calculate_entropy(text),
                      length=len(text.split()))
        
        # Classify each paragraph against reference
        classifications = []
        reference = paragraphs[0]
        
        for i in range(1, n):
            classification = self.classify_hallucination_strict(reference, paragraphs[i])
            classification['paragraph_id'] = i
            classifications.append(classification)
        
        # Build edges based on consistency
        edge_details = {}
        for i in range(n):
            for j in range(i + 1, n):
                consistency = self.calculate_pairwise_consistency(paragraphs[i], paragraphs[j])
                
                if consistency['consistency_score'] >= self.thresholds['semantic_low']:
                    G.add_edge(i, j,
                             weight=consistency['consistency_score'],
                             semantic_similarity=consistency['semantic_similarity'],
                             factual_consistency=consistency['factual_consistency'])
                    edge_details[(i, j)] = consistency
        
        # Calculate metrics
        metrics = self.calculate_metrics(G)
        
        # Calculate final consistency score for the entire set
        final_score = self.calculate_final_consistency_score(G, classifications, metrics)
        
        return G, classifications, metrics, final_score
    
    def calculate_metrics(self, G: nx.Graph) -> Dict:
        """Calculate graph metrics"""
        metrics = {
            'pagerank': nx.pagerank(G, weight='weight') if G.number_of_edges() > 0 else {n: 1/G.number_of_nodes() for n in G.nodes()},
            'betweenness': nx.betweenness_centrality(G),
            'clustering': nx.clustering(G, weight='weight'),
            'degree': dict(G.degree()),
            'components': list(nx.connected_components(G)),
            'isolated': list(nx.isolates(G))
        }
        
        return metrics
    
    def calculate_final_consistency_score(self, G: nx.Graph, classifications: List[Dict], metrics: Dict) -> float:
        """Calculate final consistency score for the entire candidate set"""
        scores = []
        weights = []
        
        # 1. Hallucination rate (lower is better)
        hallucination_rate = sum(1 for c in classifications if c['is_hallucinated']) / len(classifications)
        scores.append(1 - hallucination_rate)
        weights.append(0.3)
        
        # 2. Average semantic similarity to reference
        avg_similarity = np.mean([c['semantic_similarity'] for c in classifications])
        scores.append(avg_similarity)
        weights.append(0.2)
        
        # 3. Graph connectivity (normalized)
        if G.number_of_nodes() > 1:
            connectivity = G.number_of_edges() / (G.number_of_nodes() * (G.number_of_nodes() - 1) / 2)
        else:
            connectivity = 0
        scores.append(connectivity)
        weights.append(0.15)
        
        # 4. Average edge weight (consistency between paragraphs)
        if G.number_of_edges() > 0:
            avg_edge_weight = np.mean([d['weight'] for _, _, d in G.edges(data=True)])
        else:
            avg_edge_weight = 0
        scores.append(avg_edge_weight)
        weights.append(0.15)
        
        # 5. Component structure (prefer single large component with reference)
        largest_component = max(metrics['components'], key=len)
        if 0 in largest_component:  # Reference in largest component
            component_score = len(largest_component) / G.number_of_nodes()
        else:
            component_score = 0.5 * (len(largest_component) / G.number_of_nodes())
        scores.append(component_score)
        weights.append(0.2)
        
        # Calculate weighted final score
        final_score = np.average(scores, weights=weights)
        
        return final_score
    
    def create_interactive_visualization(self, G: nx.Graph, metrics: Dict, classifications: List[Dict],
                                       final_score: float, save_path: str = "hallucination_analysis.html") -> str:
        """Create interactive force-directed graph with final score"""
        
        # Prepare nodes data
        nodes = []
        for node in G.nodes():
            if G.nodes[node].get('is_reference', False):
                color = '#0066CC'
                status = 'REFERENCE'
            elif node in metrics['isolated']:
                color = '#666666'
                status = 'ISOLATED'
            elif node > 0 and node <= len(classifications):
                if classifications[node - 1]['is_hallucinated']:
                    color = '#FF0000'
                    status = 'HALLUCINATED'
                else:
                    # Green gradient based on PageRank
                    pr = metrics['pagerank'][node]
                    max_pr = max(v for k, v in metrics['pagerank'].items() if k != 0)
                    normalized_pr = pr / max_pr if max_pr > 0 else 0.5
                    green = int(80 + 175 * normalized_pr)
                    color = f'#{0:02x}{green:02x}{0:02x}'
                    status = 'CONSISTENT'
            else:
                color = '#666666'
                status = 'UNKNOWN'
            
            node_obj = {
                'id': node,
                'label': f'P{node}',
                'color': color,
                'status': status,
                'size': 10 + 40 * metrics['pagerank'][node],
                'pagerank': metrics['pagerank'][node],
                'degree': metrics['degree'][node],
                'text': G.nodes[node]['text']
            }
            
            if node > 0 and node <= len(classifications):
                node_obj['hallucination_score'] = classifications[node - 1]['hallucination_score']
                node_obj['semantic_similarity'] = classifications[node - 1]['semantic_similarity']
                node_obj['reasons'] = classifications[node - 1].get('reasons', [])
            
            nodes.append(node_obj)
        
        # Prepare edges data
        links = []
        for u, v, data in G.edges(data=True):
            links.append({
                'source': u,
                'target': v,
                'weight': data['weight'],
                'width': 1 + data['weight'] * 5
            })
        
        # Create HTML
        html_content = f"""
<!DOCTYPE html>
<html>
<head>
    <title>Strict Hallucination Detection Analysis</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <style>
        body {{
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f5f5f5;
        }}
        .header {{
            text-align: center;
            margin-bottom: 20px;
        }}
        .final-score {{
            font-size: 24px;
            font-weight: bold;
            padding: 20px;
            background: white;
            border-radius: 10px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            margin-bottom: 20px;
            text-align: center;
        }}
        .score-value {{
            font-size: 48px;
            color: {'#4CAF50' if final_score > 0.7 else '#FF9800' if final_score > 0.4 else '#F44336'};
        }}
        #graph {{
            background-color: white;
            border: 1px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .tooltip {{
            position: absolute;
            text-align: left;
            padding: 12px;
            font: 12px sans-serif;
            background: rgba(0, 0, 0, 0.95);
            color: white;
            border-radius: 8px;
            pointer-events: none;
            opacity: 0;
            max-width: 500px;
            line-height: 1.5;
        }}
        .legend {{
            position: absolute;
            top: 60px;
            right: 20px;
            background: rgba(255, 255, 255, 0.95);
            padding: 15px;
            border-radius: 8px;
            border: 1px solid #ddd;
        }}
        .legend-item {{
            margin: 8px 0;
            display: flex;
            align-items: center;
        }}
        .legend-color {{
            width: 20px;
            height: 20px;
            margin-right: 10px;
            border: 1px solid #333;
            border-radius: 3px;
        }}
        .controls {{
            margin-bottom: 20px;
            text-align: center;
        }}
        button {{
            padding: 10px 20px;
            margin: 0 5px;
            border: none;
            border-radius: 4px;
            background-color: #2196F3;
            color: white;
            cursor: pointer;
            font-size: 14px;
        }}
        button:hover {{
            background-color: #1976D2;
        }}
        .summary {{
            background: white;
            padding: 20px;
            border-radius: 8px;
            margin-top: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .reasons {{
            color: #d32f2f;
            font-size: 11px;
            margin-top: 5px;
        }}
    </style>
</head>
<body>
    <div class="header">
        <h1>Strict Hallucination Detection Analysis</h1>
        <div class="final-score">
            <div>Final Consistency Score</div>
            <div class="score-value">{final_score:.2%}</div>
            <div style="font-size: 16px; font-weight: normal; margin-top: 10px;">
                {self._get_score_interpretation(final_score)}
            </div>
        </div>
    </div>
    
    <div class="controls">
        <button onclick="resetSimulation()">Reset View</button>
        <button onclick="toggleLabels()">Toggle Labels</button>
        <button onclick="showSummary()">Show Summary</button>
    </div>
    
    <div id="graph"></div>
    <div class="tooltip"></div>
    
    <div class="legend">
        <h3 style="margin-top: 0;">Node Status</h3>
        <div class="legend-item">
            <div class="legend-color" style="background-color: #0066CC;"></div>
            <span>Reference (Ground Truth)</span>
        </div>
        <div class="legend-item">
            <div class="legend-color" style="background-color: #FF0000;"></div>
            <span>Hallucinated</span>
        </div>
        <div class="legend-item">
            <div class="legend-color" style="background: linear-gradient(to right, #50FF50, #00AA00);"></div>
            <span>Consistent (by PageRank)</span>
        </div>
        <div class="legend-item">
            <div class="legend-color" style="background-color: #666666;"></div>
            <span>Isolated</span>
        </div>
    </div>
    
    <div class="summary" id="summary" style="display: none;">
        <h2>Analysis Summary</h2>
        <p><strong>Total Paragraphs:</strong> {len(nodes)}</p>
        <p><strong>Hallucinated:</strong> {sum(1 for n in nodes if n['status'] == 'HALLUCINATED')} 
           ({sum(1 for n in nodes if n['status'] == 'HALLUCINATED') / (len(nodes) - 1) * 100:.1f}%)</p>
        <p><strong>Consistent:</strong> {sum(1 for n in nodes if n['status'] == 'CONSISTENT')}</p>
        <p><strong>Isolated:</strong> {sum(1 for n in nodes if n['status'] == 'ISOLATED')}</p>
        <p><strong>Graph Density:</strong> {G.number_of_edges() / (G.number_of_nodes() * (G.number_of_nodes() - 1) / 2):.3f}</p>
    </div>
    
    <script>
        const nodes = {json.dumps(nodes)};
        const links = {json.dumps(links)};
        
        const width = window.innerWidth - 60;
        const height = 600;
        
        const svg = d3.select("#graph")
            .append("svg")
            .attr("width", width)
            .attr("height", height);
        
        const zoom = d3.zoom()
            .scaleExtent([0.1, 10])
            .on("zoom", zoomed);
        
        svg.call(zoom);
        const g = svg.append("g");
        
        const tooltip = d3.select(".tooltip");
        
        const simulation = d3.forceSimulation(nodes)
            .force("link", d3.forceLink(links).id(d => d.id).distance(120).strength(d => d.weight))
            .force("charge", d3.forceManyBody().strength(-400))
            .force("center", d3.forceCenter(width / 2, height / 2))
            .force("collision", d3.forceCollide().radius(d => d.size + 10));
        
        const link = g.append("g")
            .selectAll("line")
            .data(links)
            .enter().append("line")
            .attr("stroke", "#999")
            .attr("stroke-width", d => d.width)
            .attr("stroke-opacity", 0.6);
        
        const node = g.append("g")
            .selectAll("circle")
            .data(nodes)
            .enter().append("circle")
            .attr("r", d => d.size)
            .attr("fill", d => d.color)
            .attr("stroke", "#333")
            .attr("stroke-width", 2)
            .style("cursor", "pointer")
            .call(drag(simulation));
        
        const labels = g.append("g")
            .selectAll("text")
            .data(nodes)
            .enter().append("text")
            .text(d => d.label)
            .attr("font-size", 14)
            .attr("font-weight", "bold")
            .attr("dx", d => d.size + 5)
            .attr("dy", 5)
            .style("pointer-events", "none");
        
        node.on("mouseover", function(event, d) {{
            let html = `<strong>${{d.label}} - ${{d.status}}</strong><br/>`;
            html += `<hr style="margin: 5px 0; border-color: #666;">`;
            
            if (d.status !== 'REFERENCE') {{
                html += `Hallucination Score: <strong>${{(d.hallucination_score || 0).toFixed(3)}}</strong><br/>`;
                html += `Similarity to Reference: <strong>${{(d.semantic_similarity || 0).toFixed(3)}}</strong><br/>`;
            }}
            
            html += `PageRank: ${{d.pagerank.toFixed(3)}}<br/>`;
            html += `Connections: ${{d.degree}}<br/>`;
            
            if (d.reasons && d.reasons.length > 0) {{
                html += `<div class="reasons"><strong>Issues:</strong><br/>`;
                d.reasons.forEach(r => {{
                    html += `• ${{r}}<br/>`;
                }});
                html += `</div>`;
            }}
            
            html += `<hr style="margin: 5px 0; border-color: #666;">`;
            html += `<strong>Text:</strong><br/>`;
            html += `<div style="max-height: 200px; overflow-y: auto;">${{d.text}}</div>`;
            
            tooltip.html(html)
                .style("left", (event.pageX + 10) + "px")
                .style("top", (event.pageY - 10) + "px")
                .style("opacity", 1);
            
            d3.select(this).attr("stroke-width", 4);
            
            link.style("stroke-opacity", l => 
                (l.source.id === d.id || l.target.id === d.id) ? 1 : 0.1
            );
            
            node.style("opacity", n => {{
                if (n.id === d.id) return 1;
                const connected = links.some(l => 
                    (l.source.id === d.id && l.target.id === n.id) ||
                    (l.target.id === d.id && l.source.id === n.id)
                );
                return connected ? 1 : 0.3;
            }});
        }})
        .on("mouseout", function() {{
            tooltip.style("opacity", 0);
            d3.select(this).attr("stroke-width", 2);
            link.style("stroke-opacity", 0.6);
            node.style("opacity", 1);
        }});
        
        simulation.on("tick", () => {{
            link
                .attr("x1", d => d.source.x)
                .attr("y1", d => d.source.y)
                .attr("x2", d => d.target.x)
                .attr("y2", d => d.target.y);
            
            node
                .attr("cx", d => d.x)
                .attr("cy", d => d.y);
            
            labels
                .attr("x", d => d.x)
                .attr("y", d => d.y);
        }});
        
        function drag(simulation) {{
            function dragstarted(event, d) {{
                if (!event.active) simulation.alphaTarget(0.3).restart();
                d.fx = d.x;
                d.fy = d.y;
            }}
            
            function dragged(event, d) {{
                d.fx = event.x;
                d.fy = event.y;
            }}
            
            function dragended(event, d) {{
                if (!event.active) simulation.alphaTarget(0);
                d.fx = null;
                d.fy = null;
            }}
            
            return d3.drag()
                .on("start", dragstarted)
                .on("drag", dragged)
                .on("end", dragended);
        }}
        
        function zoomed(event) {{
            g.attr("transform", event.transform);
        }}
        
        function resetSimulation() {{
            simulation.alpha(1).restart();
            svg.transition().duration(750).call(
                zoom.transform,
                d3.zoomIdentity
            );
        }}
        
        let labelsVisible = true;
        function toggleLabels() {{
            labelsVisible = !labelsVisible;
            labels.style("display", labelsVisible ? "block" : "none");
        }}
        
        function showSummary() {{
            const summary = document.getElementById('summary');
            summary.style.display = summary.style.display === 'none' ? 'block' : 'none';
        }}
    </script>
</body>
</html>
"""
        
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        return save_path
    
    def _get_score_interpretation(self, score: float) -> str:
        """Get interpretation of final score"""
        if score >= 0.8:
            return "Excellent consistency - Very few or no hallucinations detected"
        elif score >= 0.6:
            return "Good consistency - Some inconsistencies but mostly accurate"
        elif score >= 0.4:
            return "Moderate consistency - Significant inconsistencies detected"
        else:
            return "Poor consistency - Many hallucinations or contradictions"
    
    def analyze(self, reference: str, candidates: List[str], output_dir: str = ".") -> Dict:
        """Complete analysis pipeline"""
        print("\n" + "="*60)
        print("STRICT HALLUCINATION DETECTION ANALYSIS")
        print("="*60)
        
        # Combine paragraphs
        all_paragraphs = [reference] + candidates
        
        # Build graph and analyze
        print("\nAnalyzing paragraphs...")
        G, classifications, metrics, final_score = self.build_graph_and_analyze(all_paragraphs)
        
        # Create visualization
        print("Creating interactive visualization...")
        viz_path = os.path.join(output_dir, "strict_hallucination_analysis.html")
        self.create_interactive_visualization(G, metrics, classifications, final_score, viz_path)
        
        # Print detailed results
        print("\n" + "-"*60)
        print("ANALYSIS RESULTS")
        print("-"*60)
        print(f"\nFINAL CONSISTENCY SCORE: {final_score:.2%}")
        print(f"Interpretation: {self._get_score_interpretation(final_score)}")
        
        print(f"\nTotal paragraphs: {len(all_paragraphs)}")
        print(f"Hallucinated: {sum(1 for c in classifications if c['is_hallucinated'])} / {len(classifications)}")
        print(f"Consistent: {sum(1 for c in classifications if not c['is_hallucinated'])} / {len(classifications)}")
        print(f"Graph connections: {G.number_of_edges()}")
        print(f"Isolated nodes: {len(metrics['isolated'])}")
        
        # Show hallucination details
        print("\nHALLUCINATION DETAILS:")
        for i, classification in enumerate(classifications):
            if classification['is_hallucinated']:
                print(f"\nParagraph {i+1}:")
                print(f"  Score: {classification['hallucination_score']:.3f}")
                print(f"  Reasons: {', '.join(classification['reasons'])}")
        
        print(f"\nVisualization saved to: {viz_path}")
        print("Opening in browser...")
        webbrowser.open(f'file://{os.path.abspath(viz_path)}')
        
        return {
            'graph': G,
            'metrics': metrics,
            'classifications': classifications,
            'final_score': final_score,
            'visualization': viz_path
        }


# Example usage
if __name__ == "__main__":
    # Initialize detector
    detector = StrictHallucinationDetector(use_gpu=False)
    
    # Example with clear test cases
    reference = """The company reported revenue of $2.5 million in Q4 2023, with a 15% increase from the previous quarter. 
    CEO John Smith announced expansion plans on January 15, 2024, targeting the Asian market. 
    The profit margin improved to 22% due to cost optimization strategies."""
    
    candidates = [
        # Should be CONSISTENT - exact same facts
        """Q4 2023 revenue reached $2.5 million, marking 15% quarterly growth. 
        John Smith revealed Asian market expansion on January 15, 2024. 
        Profit margins rose to 22% through cost optimization.""",
        
        # Should be HALLUCINATED - wrong numbers
        """The company earned $3.2 million in Q4 2023, a 20% increase. 
        CEO John Smith discussed expansion plans on January 20, 2024. 
        Profit margins reached 25% after cost controls.""",
        
        # Should be CONSISTENT - adds details without contradicting
        """In Q4 2023, the company saw $2.5 million in revenue. 
        The January 15, 2024 announcement by CEO John Smith highlighted Asian expansion. 
        Additionally, employee satisfaction increased alongside the 22% profit margin.""",
        
        # Should be HALLUCINATED - contradicts facts
        """Q4 2023 saw losses of $1.8 million, down 30% from Q3. 
        CEO John Smith announced on January 15, 2024 that expansion plans were cancelled. 
        Profit margins fell to 12% due to rising costs.""",
        
        # Should be CONSISTENT - rephrases same information
        """Financial results: $2.5M revenue in Q4 2023 (15% QoQ growth). 
        Asian expansion announced by John Smith on Jan 15, 2024. 
        22% profit margin achieved via cost optimization.""",
        
        # Should be ISOLATED - unrelated content
        """The weather forecast predicts sunny skies for the weekend. 
        Temperature will reach 75°F with low humidity. 
        Perfect conditions for outdoor activities."""
    ]
    
    # Run analysis
    results = detector.analyze(reference, candidates)
