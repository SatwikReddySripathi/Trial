import plotly.graph_objects as go
import networkx as nx
import numpy as np

# Initialize graph
G = nx.Graph()

# Add nodes with classification types and color coding
nodes_data = [
    {"id": 0, "label": "Reference", "type": "reference", "color": "blue"},
    {"id": 1, "label": "P1", "type": "consistent", "color": "green"},
    {"id": 2, "label": "P2", "type": "contradiction", "color": "darkred"},
    {"id": 3, "label": "P3", "type": "factual_error", "color": "firebrick"},
    {"id": 4, "label": "P4", "type": "omission", "color": "lightcoral"},
    {"id": 5, "label": "P5", "type": "extra", "color": "gray"}
]

for node in nodes_data:
    G.add_node(node["id"], **node)

# Add sample edges (reference to each)
edges = [(0, i) for i in range(1, 6)]
G.add_edges_from(edges)

# Generate 3D layout
pos = nx.spring_layout(G, dim=3, seed=42)
for n in G.nodes:
    G.nodes[n]['pos'] = pos[n]

# Create node trace
node_trace = go.Scatter3d(
    x=[G.nodes[n]['pos'][0] for n in G.nodes()],
    y=[G.nodes[n]['pos'][1] for n in G.nodes()],
    z=[G.nodes[n]['pos'][2] for n in G.nodes()],
    mode='markers+text',
    text=[G.nodes[n]['label'] for n in G.nodes()],
    textposition='bottom center',
    marker=dict(
        size=12,
        color=[G.nodes[n]['color'] for n in G.nodes()],
        line=dict(width=2, color='black')
    ),
    hovertext=[
        f"{G.nodes[n]['label']}<br>"
        f"Type: {G.nodes[n]['type'].replace('_',' ').title()}<br>"
        f"Semantic Similarity: {0.6 + 0.05*n:.2f}<br>"
        f"Factual Consistency: {1.0 - 0.2*n:.2f}<br>"
        f"Entailment: {0.7 - 0.1*n:.2f}<br>"
        f"Contradiction: {0.2 + 0.15*n:.2f}<br>"
        f"Confidence: {0.9 - 0.1*n:.2f}"
        for n in G.nodes()
    ],
    hoverinfo="text"
)

# Create edge trace
edge_x, edge_y, edge_z = [], [], []
for edge in G.edges():
    x0, y0, z0 = G.nodes[edge[0]]['pos']
    x1, y1, z1 = G.nodes[edge[1]]['pos']
    edge_x += [x0, x1, None]
    edge_y += [y0, y1, None]
    edge_z += [z0, z1, None]

edge_trace = go.Scatter3d(
    x=edge_x, y=edge_y, z=edge_z,
    mode='lines',
    line=dict(color='lightgray', width=2),
    hoverinfo='none'
)

# Layout with legend and scores
fig = go.Figure(data=[edge_trace, node_trace])

fig.update_layout(
    title='Graph Based Reliability Scorer',
    margin=dict(l=20, r=20, b=20, t=60),
    showlegend=False,
    scene=dict(
        xaxis=dict(showgrid=False, zeroline=False),
        yaxis=dict(showgrid=False, zeroline=False),
        zaxis=dict(showgrid=False, zeroline=False)
    ),
    annotations=[
        dict(text="Overall Reliability: <b>65.9%</b>", x=0, y=1.1, showarrow=False, xref="paper", yref="paper"),
        dict(text="Logical Consistency: <b>93.1%</b>", x=0.33, y=1.1, showarrow=False, xref="paper", yref="paper"),
        dict(text="Semantic Coherence: <b>63.2%</b>", x=0.66, y=1.1, showarrow=False, xref="paper", yref="paper"),
        dict(text="Info Completeness: <b>40.0%</b>", x=1.0, y=1.1, showarrow=False, xref="paper", yref="paper")
    ]
)

fig.show()







Slide 2: Motivation
	‚Ä¢	Problem: Hallucinations in LLMs undermine trust in generative NLP
	‚Ä¢	Challenge: Existing methods are piecemeal ‚Äî they detect either factual errors or entailment issues, not both
	‚Ä¢	Goal: Build a comprehensive system that detects all hallucination types (factual, contradictory, misleading, omitted)

Visual:
Diagram showing three overlapping circles: Factual Consistency, Entailment, Semantic Similarity ‚Üí hallucination detection in the center

‚∏ª

üö® Slide 3: Hallucination Types (Taxonomy)
	‚Ä¢	Factual Error: Wrong numbers, names, dates
	‚Ä¢	Contradiction: Opposes known or stated information
	‚Ä¢	Omission: Leaves out critical facts
	‚Ä¢	Misleading/Fabrication: Uses same entities with unrelated facts

Visual: Table or icons showing each type with examples


üîç Slide 5: Core Modules

Break down the 3 detection modules:
	‚Ä¢	Semantic Similarity
	‚Ä¢	Embeddings + TF-IDF + Word Overlap
	‚Ä¢	Factual Consistency
	‚Ä¢	Named entities, numbers, money, dates, actions
	‚Ä¢	Entailment
	‚Ä¢	Bidirectional DeBERTa-based NLI

Visual: Table or side-by-side comparison of modules

‚∏ª

üß† Slide 6: Classifier Logic

How decisions are made:
	‚Ä¢	High similarity + factual errors ‚Üí factual hallucination
	‚Ä¢	High similarity + contradictions ‚Üí contradiction
	‚Ä¢	Low similarity + same entities ‚Üí misleading
	‚Ä¢	Missing facts ‚Üí omission

Visual: Flowchart of decision tree or example-based logic map

‚∏ª

üåê Slide 7: Graph-Based Framework
	‚Ä¢	Nodes: Paragraphs
	‚Ä¢	Edges: Consistency (semantic + factual)
	‚Ä¢	Metrics:
	‚Ä¢	PageRank ‚Üí importance
	‚Ä¢	Centrality ‚Üí coherence
	‚Ä¢	Isolation ‚Üí hallucination risk

Visual: Sample graph with colored nodes (consistent vs. hallucinated)

‚∏ª

üìä Slide 8: Scoring Metrics

Show the final scores:
	‚Ä¢	Factual Accuracy
	‚Ä¢	Logical Consistency
	‚Ä¢	Semantic Coherence
	‚Ä¢	Information Completeness
	‚Ä¢	Overall Reliability (weighted)

Visual: Radar chart or stacked bar chart

‚∏ª

üìÅ Slide 9: Visualization Demo

Embed screenshots or link to interactive visualization:
	‚Ä¢	D3-based HTML graph
	‚Ä¢	Tooltip with paragraph-level analysis
	‚Ä¢	Color legend for hallucination types

Optional: Short video demo or gif

‚∏ª

üß™ Slide 10: Example Results


table


Slide 11: Why This Matters
	‚Ä¢	Unified approach: no need for separate tools for fact-checking, NLI, or semantic match
	‚Ä¢	Scalable, modular, explainable
	‚Ä¢	Applicable to summarization, QA, dialogue, RAG systems

Visual: Application icons or domains (chatbots, news gen, etc.)

‚∏ª

üõ†Ô∏è Slide 12: Future Work
	‚Ä¢	Integrate retrieval grounding (RAG context)
	‚Ä¢	Extend to multimodal (text + image)
	‚Ä¢	Automate feedback loop for self-correction in generation

‚∏ª

üì¨ Slide 13: Thank You / Q&A
	‚Ä¢	Contact Info
	‚Ä¢	GitHub / Live Demo Link (if available)