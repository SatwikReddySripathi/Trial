Slide 2: Motivation
	â€¢	Problem: Hallucinations in LLMs undermine trust in generative NLP
	â€¢	Challenge: Existing methods are piecemeal â€” they detect either factual errors or entailment issues, not both
	â€¢	Goal: Build a comprehensive system that detects all hallucination types (factual, contradictory, misleading, omitted)

Visual:
Diagram showing three overlapping circles: Factual Consistency, Entailment, Semantic Similarity â†’ hallucination detection in the center

â¸»

ðŸš¨ Slide 3: Hallucination Types (Taxonomy)
	â€¢	Factual Error: Wrong numbers, names, dates
	â€¢	Contradiction: Opposes known or stated information
	â€¢	Omission: Leaves out critical facts
	â€¢	Misleading/Fabrication: Uses same entities with unrelated facts

Visual: Table or icons showing each type with examples