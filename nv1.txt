"""
Improved Hallucination Detection Logic
====================================
A more sophisticated approach to detecting hallucinations in text
"""

import numpy as np
from typing import List, Dict, Tuple, Set, Optional
from dataclasses import dataclass
from enum import Enum
import re
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import spacy
from dateutil import parser
import torch


class HallucinationType(Enum):
    """Types of hallucinations"""
    NONE = "none"
    FACTUAL_CONTRADICTION = "factual_contradiction"  # Direct conflict with facts
    SEMANTIC_DRIFT = "semantic_drift"  # On topic but inventing details
    CONTEXT_MIXING = "context_mixing"  # Combining unrelated facts
    FABRICATION = "fabrication"  # Adding unsupported details
    TEMPORAL_INCONSISTENCY = "temporal_inconsistency"  # Timeline conflicts
    QUANTITATIVE_ERROR = "quantitative_error"  # Number/measurement errors


@dataclass
class Entity:
    """Enhanced entity representation"""
    text: str
    normalized: str
    type: str
    value: Optional[float] = None
    context: Optional[str] = None
    confidence: float = 1.0


@dataclass
class Claim:
    """Represents a factual claim"""
    text: str
    entities: List[Entity]
    subject: Optional[str] = None
    predicate: Optional[str] = None
    object: Optional[str] = None
    confidence: float = 1.0
    source_span: Tuple[int, int] = (0, 0)


@dataclass
class HallucinationResult:
    """Detailed hallucination analysis result"""
    is_hallucinated: bool
    hallucination_types: List[HallucinationType]
    confidence: float
    evidence: Dict[str, any]
    supported_claims: List[Claim]
    contradicted_claims: List[Claim]
    unsupported_claims: List[Claim]
    explanation: str


class ImprovedHallucinationDetector:
    """Advanced hallucination detection with better logic"""
    
    def __init__(self, use_gpu=False):
        self.device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'
        
        # Models
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.nli_model = pipeline(
            "text-classification",
            model="microsoft/deberta-v3-large-mnli",
            device=0 if self.device == 'cuda' else -1
        )
        
        # SpaCy for better NLP
        self.nlp = spacy.load("en_core_web_sm")
        
        # Enhanced patterns
        self.patterns = {
            'date': [
                r'\b(?:\d{1,2}[-/]\d{1,2}[-/]\d{2,4})\b',
                r'\b(?:\d{4}[-/]\d{1,2}[-/]\d{1,2})\b',
                r'\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \d{1,2},? \d{4}\b',
                r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December) \d{1,2},? \d{4}\b'
            ],
            'money': [
                r'\$[\d,]+(?:\.\d{2})?(?:\s*(?:million|billion|thousand|M|B|K))?',
                r'\b\d+(?:,\d{3})*(?:\.\d{2})?\s*(?:dollars?|USD|euros?|EUR|pounds?|GBP)\b'
            ],
            'percentage': [
                r'\b\d+(?:\.\d+)?%',
                r'\b\d+(?:\.\d+)?\s*percent\b'
            ],
            'quantity': [
                r'\b\d+(?:,\d{3})*(?:\.\d+)?\s*(?:units?|items?|people|customers?|employees?)\b'
            ]
        }
    
    def extract_entities_advanced(self, text: str) -> List[Entity]:
        """Extract entities with normalization"""
        entities = []
        
        # Date extraction with normalization
        for pattern in self.patterns['date']:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                date_text = match.group()
                try:
                    normalized = parser.parse(date_text).strftime('%Y-%m-%d')
                    entities.append(Entity(
                        text=date_text,
                        normalized=normalized,
                        type='DATE',
                        context=text[max(0, match.start()-20):min(len(text), match.end()+20)]
                    ))
                except:
                    pass
        
        # Money extraction with value parsing
        for pattern in self.patterns['money']:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                money_text = match.group()
                value = self._parse_money_value(money_text)
                if value:
                    entities.append(Entity(
                        text=money_text,
                        normalized=f"${value:.2f}",
                        type='MONEY',
                        value=value,
                        context=text[max(0, match.start()-20):min(len(text), match.end()+20)]
                    ))
        
        # Percentage extraction
        for pattern in self.patterns['percentage']:
            for match in re.finditer(pattern, text, re.IGNORECASE):
                pct_text = match.group()
                value = float(re.findall(r'\d+(?:\.\d+)?', pct_text)[0])
                entities.append(Entity(
                    text=pct_text,
                    normalized=f"{value}%",
                    type='PERCENTAGE',
                    value=value,
                    context=text[max(0, match.start()-20):min(len(text), match.end()+20)]
                ))
        
        # SpaCy NER for additional entities
        doc = self.nlp(text)
        for ent in doc.ents:
            if ent.label_ in ['PERSON', 'ORG', 'GPE', 'PRODUCT']:
                entities.append(Entity(
                    text=ent.text,
                    normalized=ent.text.lower(),
                    type=ent.label_,
                    context=text[max(0, ent.start_char-20):min(len(text), ent.end_char+20)]
                ))
        
        return entities
    
    def _parse_money_value(self, money_str: str) -> Optional[float]:
        """Parse money string to float value"""
        try:
            # Remove currency symbols and spaces
            clean = re.sub(r'[$,\s]', '', money_str)
            
            # Handle multipliers
            multipliers = {
                'thousand': 1000, 'k': 1000,
                'million': 1000000, 'm': 1000000,
                'billion': 1000000000, 'b': 1000000000
            }
            
            for word, mult in multipliers.items():
                if word in money_str.lower():
                    clean = re.sub(rf'\s*{word}.*', '', clean, flags=re.IGNORECASE)
                    return float(clean) * mult
            
            return float(clean)
        except:
            return None
    
    def extract_claims(self, text: str) -> List[Claim]:
        """Extract factual claims from text"""
        claims = []
        doc = self.nlp(text)
        
        for sent in doc.sents:
            # Extract entities in this sentence
            sent_entities = [e for e in self.extract_entities_advanced(sent.text)]
            
            if sent_entities:
                # Simple dependency parsing for SVO triples
                subject = None
                predicate = None
                obj = None
                
                for token in sent:
                    if token.dep_ == "nsubj":
                        subject = token.text
                    elif token.pos_ == "VERB":
                        predicate = token.text
                    elif token.dep_ in ["dobj", "attr"]:
                        obj = token.text
                
                claims.append(Claim(
                    text=sent.text.strip(),
                    entities=sent_entities,
                    subject=subject,
                    predicate=predicate,
                    object=obj,
                    source_span=(sent.start_char, sent.end_char)
                ))
        
        return claims
    
    def check_claim_consistency(self, ref_claim: Claim, cand_claim: Claim) -> Dict:
        """Check if two claims are consistent"""
        result = {
            'consistent': True,
            'contradiction_type': None,
            'confidence': 1.0,
            'evidence': {}
        }
        
        # Check entity contradictions
        for ref_ent in ref_claim.entities:
            for cand_ent in cand_claim.entities:
                if ref_ent.type == cand_ent.type and ref_ent.context and cand_ent.context:
                    # Similar context but different values?
                    context_sim = self._semantic_similarity(ref_ent.context, cand_ent.context)
                    
                    if context_sim > 0.7:  # Talking about same thing
                        if ref_ent.type in ['DATE', 'MONEY', 'PERCENTAGE']:
                            if ref_ent.normalized != cand_ent.normalized:
                                result['consistent'] = False
                                result['contradiction_type'] = HallucinationType.FACTUAL_CONTRADICTION
                                result['evidence'] = {
                                    'reference': ref_ent.text,
                                    'candidate': cand_ent.text,
                                    'type': ref_ent.type
                                }
                                return result
        
        # Check semantic contradiction using NLI
        nli_result = self.nli_model(f"{ref_claim.text} [SEP] {cand_claim.text}")
        contradiction_score = next((r['score'] for r in nli_result if r['label'] == 'CONTRADICTION'), 0)
        
        if contradiction_score > 0.8:
            result['consistent'] = False
            result['contradiction_type'] = HallucinationType.SEMANTIC_DRIFT
            result['confidence'] = contradiction_score
        
        return result
    
    def _semantic_similarity(self, text1: str, text2: str) -> float:
        """Calculate semantic similarity"""
        embeddings = self.sentence_model.encode([text1, text2])
        return float(np.dot(embeddings[0], embeddings[1]) / 
                    (np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])))
    
    def analyze_hallucination(self, reference: str, candidate: str) -> HallucinationResult:
        """Main hallucination analysis with improved logic"""
        # Extract claims from both texts
        ref_claims = self.extract_claims(reference)
        cand_claims = self.extract_claims(candidate)
        
        supported_claims = []
        contradicted_claims = []
        unsupported_claims = []
        hallucination_types = set()
        
        # Check each candidate claim
        for cand_claim in cand_claims:
            is_supported = False
            is_contradicted = False
            
            for ref_claim in ref_claims:
                # Check semantic relevance first
                claim_sim = self._semantic_similarity(ref_claim.text, cand_claim.text)
                
                if claim_sim > 0.5:  # Related claims
                    consistency = self.check_claim_consistency(ref_claim, cand_claim)
                    
                    if not consistency['consistent']:
                        is_contradicted = True
                        contradicted_claims.append(cand_claim)
                        hallucination_types.add(consistency['contradiction_type'])
                        break
                    elif claim_sim > 0.8:  # High similarity = supported
                        is_supported = True
                        supported_claims.append(cand_claim)
                        break
            
            if not is_supported and not is_contradicted:
                # Check if it's a reasonable inference or fabrication
                if self._is_reasonable_inference(reference, cand_claim):
                    supported_claims.append(cand_claim)
                else:
                    unsupported_claims.append(cand_claim)
                    hallucination_types.add(HallucinationType.FABRICATION)
        
        # Determine overall hallucination status
        total_claims = len(cand_claims) if cand_claims else 1
        contradiction_ratio = len(contradicted_claims) / total_claims
        unsupported_ratio = len(unsupported_claims) / total_claims
        
        is_hallucinated = (contradiction_ratio > 0.2 or 
                          unsupported_ratio > 0.5 or 
                          HallucinationType.FACTUAL_CONTRADICTION in hallucination_types)
        
        confidence = min(contradiction_ratio + unsupported_ratio * 0.5, 1.0)
        
        # Generate explanation
        explanation = self._generate_explanation(
            supported_claims, contradicted_claims, unsupported_claims, hallucination_types
        )
        
        return HallucinationResult(
            is_hallucinated=is_hallucinated,
            hallucination_types=list(hallucination_types),
            confidence=confidence,
            evidence={
                'total_claims': total_claims,
                'supported': len(supported_claims),
                'contradicted': len(contradicted_claims),
                'unsupported': len(unsupported_claims)
            },
            supported_claims=supported_claims,
            contradicted_claims=contradicted_claims,
            unsupported_claims=unsupported_claims,
            explanation=explanation
        )
    
    def _is_reasonable_inference(self, reference: str, claim: Claim) -> bool:
        """Check if a claim is a reasonable inference from reference"""
        # Use NLI to check if reference entails the claim
        nli_result = self.nli_model(f"{reference} [SEP] {claim.text}")
        entailment_score = next((r['score'] for r in nli_result if r['label'] == 'ENTAILMENT'), 0)
        neutral_score = next((r['score'] for r in nli_result if r['label'] == 'NEUTRAL'), 0)
        
        # High entailment or high neutral (non-contradictory) is reasonable
        return entailment_score > 0.6 or neutral_score > 0.7
    
    def _generate_explanation(self, supported: List[Claim], contradicted: List[Claim],
                            unsupported: List[Claim], types: Set[HallucinationType]) -> str:
        """Generate human-readable explanation"""
        explanation = []
        
        if contradicted:
            explanation.append(f"Found {len(contradicted)} contradictions:")
            for claim in contradicted[:2]:  # Show first 2
                explanation.append(f"  - '{claim.text}'")
        
        if unsupported:
            explanation.append(f"Found {len(unsupported)} unsupported claims:")
            for claim in unsupported[:2]:
                explanation.append(f"  - '{claim.text}'")
        
        if HallucinationType.FACTUAL_CONTRADICTION in types:
            explanation.append("Critical: Direct factual contradictions detected")
        
        if not contradicted and not unsupported:
            explanation.append("All claims are supported or reasonably inferred")
        
        return "\n".join(explanation)


# Example usage showing improved logic
if __name__ == "__main__":
    detector = ImprovedHallucinationDetector()
    
    reference = """The company reported revenue of $2.5 million in Q4 2023, with a 15% increase from the previous quarter. 
    CEO John Smith announced expansion plans on January 15, 2024, targeting the Asian market."""
    
    # Test various types of hallucinations
    test_cases = [
        ("Factual contradiction", 
         "The company reported revenue of $3.2 million in Q4 2023, with a 15% increase from the previous quarter."),
        
        ("Additional non-contradictory info",
         "The company reported revenue of $2.5 million in Q4 2023. The strong performance was driven by increased sales in the retail sector."),
        
        ("Semantic drift",
         "John Smith announced that the company would focus on European markets in January 2024, following the Q4 results."),
        
        ("Complete fabrication",
         "The company also announced a new product line that would launch in March 2024, expected to generate additional revenue."),
        
        ("Reasonable inference",
         "With $2.5 million in Q4 2023 revenue and 15% growth, the company appears to be performing well financially.")
    ]
    
    for name, candidate in test_cases:
        result = detector.analyze_hallucination(reference, candidate)
        print(f"\n{name}:")
        print(f"  Hallucinated: {result.is_hallucinated}")
        print(f"  Types: {[t.value for t in result.hallucination_types]}")
        print(f"  Confidence: {result.confidence:.2f}")
        print(f"  Explanation:\n    {result.explanation.replace(chr(10), chr(10) + '    ')}")
